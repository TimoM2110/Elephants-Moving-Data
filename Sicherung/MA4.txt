\documentclass[12pt,paper=A4,titlepage,bibliography=totoc,numbers=noenddot]{report}

\makeatletter
\DeclareOldFontCommand{\rm}{\normalfont\rmfamily}{\mathrm}
\DeclareOldFontCommand{\sf}{\normalfont\sffamily}{\mathsf}
\DeclareOldFontCommand{\tt}{\normalfont\ttfamily}{\mathtt}
\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}
\DeclareOldFontCommand{\it}{\normalfont\itshape}{\mathit}
\DeclareOldFontCommand{\sl}{\normalfont\slshape}{\@nomath\sl}
\DeclareOldFontCommand{\sc}{\normalfont\scshape}{\@nomath\sc}
\makeatother

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
%\usepackage[ngerman]{babel}
\usepackage{booktabs} % Tipp: www.tablesgenerator.com 
\usepackage[format=plain,justification=centering]{caption}
% \usepackage{dcolumn}
\usepackage{enumitem}
\usepackage{exscale}
\usepackage{flafter}
\usepackage[T1]{fontenc} 
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref} \urlstyle{same}
\usepackage{icomma}
%\usepackage[ansinew]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{gensymb}
\usepackage{bm}
\usepackage{pdfpages}
\usepackage{natbib}

\usepackage[scaled]{helvet}
\usepackage{hyperref}


\usepackage{afterpage}

\usepackage{verbatim}

%table
\usepackage{geometry}
\usepackage{siunitx}
\usepackage{booktabs, makecell}
\usepackage[referable]{threeparttablex}

% \usepackage{longtable}
\usepackage{mathtools} \mathtoolsset{showmanualtags}
\usepackage{apalike}
\usepackage{nccmath}
% \usepackage{rotating}
\usepackage{setspace} \onehalfspacing
% \usepackage[notref,notcite]{showkeys}
% \usepackage{tabularx}
\usepackage{textcomp}
% \usepackage[normalem]{ulem}
% \usepackage{xcolor}
\usepackage{xfrac}
\usepackage{xspace}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{chngcntr}
\numberwithin{equation}{chapter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage[headsepline]{scrlayer-scrpage}
%\automark{section}
%\automark*{subsection}
%\clearpairofpagestyles
%\ohead{\headmark}
%\rofoot{\pagemark}

\usepackage[bottom,hang,flushmargin]{footmisc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{acronym}

%\DeclareAcronym{eu}{
%  short=EU,
%  long=European Union,}
 
\usepackage{fancyhdr}

\makeatletter

\def\@makechapterhead#1{%
  \vspace*{10\p@}%
%  {\parindent \z@ \centering \reset@font
  {\parindent \z@ \reset@font        
		\par\nobreak
        \vspace*{2\p@}%
        {\Huge \bfseries \thechapter\quad #1\par\nobreak}
        \par\nobreak
        \vspace*{2\p@}%
    \vskip 40\p@
    %\vskip 100\p@
  }}
\def\@makeschapterhead#1{%
  \vspace*{10\p@}%
  {\parindent \z@ \reset@font
        \par\nobreak
        \vspace*{2\p@}%
        {\Huge \bfseries #1\par\nobreak}
        \par\nobreak
        \vspace*{2\p@}%
    \vskip 40\p@
  }}
\makeatother


\pagestyle{fancy}

\renewcommand{\chaptermark}[1]{ \markboth{#1}{} }
\fancyhead{} % clear all fields
\fancyfoot{}

\DeclareMathOperator*{\argmax}{\arg\!\max}

% \bibliographystyle{natdin}  
\bibliographystyle{apalike}  

% Palatino
\usepackage{mathpazo} 
\usepackage[scale=1.0425]{tgpagella}
\usepackage[scale=.95]{tgheros}
\usepackage{tgcursor}

% Datum
\usepackage[ngerman]{datetime}

\newdateformat{myformat}{\THEDAY{ten }\monthnamengerman[\THEMONTH], \THEYEAR}

% Formatierung der Fußnoten
%\makeatletter \renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1 \or * \or ** \or *** \or %\dagger \or \ddagger \or \dagger\dagger \or \ddagger\ddagger \fi}}\makeatother
%\deffootnote{1.5em}{1em}{\makebox[1.5em][l]{\textsuperscript{\thefootnotemark}}}

%R
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\proglang=\textsf
\let\code=\texttt

%Formatierung des Anhangs
\newcommand*\appendixmore{
  \clearpage
  \addsec{\text{Appendix}}
  \renewcommand{\thesection}{\text{A}\arabic{section}}%
}
%Captions
%\captionsetup{justification=raggedright,singlelinecheck=false, format=hang}

\usepackage{etoolbox}

\fancyhead[RO]{\textit{\thechapter \quad \leftmark}}
\fancyfoot[C]{\thepage}

\usepackage{verbatimbox}

%\BeforeStartingTOC[toc]{\pagestyle{plain}}
%\AfterStartingTOC[toc]{\clearpage}

\begin{document} \setlength{\emergencystretch}{3em} 

\pagenumbering{roman}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % % % % % % % % % % % % % % % % % % % % 
% Title page!!!!!
\newgeometry{margin=0.5in}
\begin{center}
\thispagestyle{empty}


\Huge{Universit\"at Bielefeld} \\


\vspace{.5cm}
\Large{Fakult\"at f\"ur Wirtschaftswissenschaften} 

\vspace{1cm}

\large{{\textbf{Masterarbeit} }} 
\vspace{0.2cm}

\normalsize{im Studiengang Wirtschaftswissenschaften}

\vspace{1cm}

\normalsize{zum Thema:} 
\large

\vspace{0.25cm}
{\textbf{Statistical modeling of tracking data \text{\normalfont--} \\ an empirical comparison of methods}}




\vspace{1cm}
\normalsize
vorgelegt von \\
\vspace{0.25cm}
\large
{\textbf{Timo Meyer}}\\
\vspace{0.5cm}
\normalsize
\begin{tabular}{ll}
Matrikel-Nr:  & 2729188\\
Anschrift:  & Nagelsholz 11, 32139 Spenge
\end{tabular}
\vspace{1.5cm}

\normalsize{ausgef\"uhrt zum Zwecke der Erlangung des akademischen Grades Master der Wirtschaftswissenschaften (M.Sc.)}  

\vspace{1cm}
\normalsize
\begin{tabular}{ll} 
1. Pr\"ufer/in  & Prof. Dr. Roland Langrock \\ & (Lehrstuhl Statistik und Datenanalyse)\\
2. Pr\"ufer/in  & Prof. Dr. Dietmar Bauer \\ & (Lehrstuhl \"Okonometrie)
\end{tabular}

\end{center}

\vspace{2cm}
Bielefeld, im Juni 2022 


\newpage
\restoregeometry 
% % % % % % % % % % % % % % % % % % % % % % % % 
\setcounter{page}{1}

\tableofcontents 

\clearpage


\listoffigures \addcontentsline{toc}{section}{List of Figures}

\listoftables \addcontentsline{toc}{section}{List of Tables}

\section*{List of Symbols} \addcontentsline{toc}{section}{List of Symbols}

{\large \textbf{Latin}}

\begin{enumerate}[label=Abschnitt \theenumi:, leftmargin=*, align=left]
\setlength{\labelsep}{5pt}
\setlength\itemsep{-3.7pt}
\item[$b_i (\cdot)\in {[0 ; 1]}$ ] Einschätzung des Managers $m$ über die Wahrscheinlichkeit, dass Agent $i$ der Arbeitsnorm $\hat{e}$ nicht nachkommt
\item[$c_i (e_i)$] Kosten im Sinne von Arbeitsleid des Agenten $i$ bei Arbeitsleistung $e_i$
\item[$d_i$] Einschätzung des Spielers $i$ darüber, was Spieler $j \neq i$ glaubt, dass Spieler $i$ spielt.
\item[$\tilde{e}, \hat{e}, e \in \mathbb{R}_{\geq 0}^{n}$] Arbeitsleistung der Agenten 
\item[$\bar{e}$] Arbeitsnorm in der Unternehmung bzw. Arbeitsgruppe
\item[$e^{fb}$] first-best Arbeitsleistung
\item[$e^*$] Arbeitsleistung aus der Nash-Verhandlungslösung
\item[$e^M$] Arbeitsleistung bei (perfektem) Monitoring
\item[$f (\cdot)$] Signal des Managers über Arbeitsleistung des Agenten 
\item[$g(e) = x$] Funktion des gemeinsam erbrachten Outputs $x$ der Agenten
\item[$i, j$] Index der Agenten mit $i , j \in (1, \dots, n)$ und $n \geq 2$
\item[$K$] Kontrollkosten des (perfekten) Monitorings durch den Prinzipal
\item[$m$] Index des Managers
\item[$o (\cdot)$] Gefälligkeitsfunktion
\item[$p (\cdot)$] Kosten im Sinne von Arbeitsleid des Managers aus der Sanktion des Agenten
\item[$q$] (konstante) Grenzproduktivität der Agenten
\item[$R_i$] Anzahl Reports, die der Manager über einen Agenten $i$ erhält
\item[$r_{ji}$] Frequenz, mit der Agent $j$ Agent $i$ meldet
\item[$r_{ji}^*$] gleichgewichtige Frequenz, mit der Agent $j$ Agent $i$ meldet, Nash-Lösung
\item[$S_i (\cdot)$] Sanktion, die der Managers dem Agenten $i$ auferlegt
\item[$S_i^* (\cdot)$] gleichgewichtige Sanktion die der Manager dem Agenten $i$ auferlegt
\item[$s$] für alle Agenten identischer Anteil am Output
\item[$t (r_{ji})$] Arbeitsleid des Agenten $j$ durch das Melden von Agent $i$
\item[$U (\cdot)$] Nutzenfunktion
\item[$w$] Fixum 
\item[$z_i$] Entlohnung des Agenten $i$, sofern der Prinzipal die Höhe des Outputs verifiziert hat
\item[] 
\item[{\large \textbf{Greek}}]
\end{enumerate}

\begin{enumerate}[label=Abschnitt \theenumi:, leftmargin=*, align=left]
\setlength{\labelsep}{5pt}
\setlength\itemsep{-3.7pt}
\item[$\Gamma$] Normalform eines Spiels
\item[$\Delta_i$] Konstante als Anteil des Agenten $i$ an Steigerung des Outputs
\item[$\lambda$] Reziprozitätsparameter
\item[$\mu_i (\cdot), \tilde{\mu}_i (\cdot)$] Entlohnungsfunktion des Agenten $i$
\item[$\pi (\cdot)$] Payoff, basierend auf materiellen Größen
\item[$\tau_i$] Einschätzung eines Spielers über die Strategie des Spielers $i$
\item[$\varsigma_j (\cdot)$] Nutzeneffekt des Agenten $j$ aufgrund einer von der Norm abweichenden Arbeitsleistung
\item[$\Omega$] Strategieraum
\item[$\omega_i \in \Omega_i$] Strategie aus der Strategiemenge des Spielers $i$
\end{enumerate}
\thispagestyle{plain}
\section*{Acronyms} \addcontentsline{toc}{section}{Acronyms}

\begin{enumerate}[label=Abschnitt \theenumi:, leftmargin=*, align=left]
\setlength{\labelsep}{5pt}
\setlength\itemsep{-3.5pt}
\item[ACF] Autocorrelation Function
\item[CTCRW] Continuous-time Correlated Random Walk
\item[EDA] Exploratory Data Analysis
\item[EDF] Effective Degrees of Freedom
\item[EM] Expectation-Maximisation
\item[GAM] Generalised Additive Model
\item[GAMM] Generalised Additive Mixed Model
\item[GCV] Generalised Cross Validation
\item[HMM] Hidden Markov Model
\item[LMM] Linear Mixed Model
\item[MCMC] Markov Chain Monte Carlo
\item[PIRLS] Penalised Iterative Re-Weighted Least Squares
\item[REML] Restricted Maximum Likelihood Estimation
\item[SDE] Stochastic Differential Equation
\item[SSM] State-Space Model
\end{enumerate} \thispagestyle{plain}

\clearpage \pagenumbering{arabic} \setcounter{page}{1}
\pagestyle{fancy}
% % % % % % % % % % % % % % % % % % % % % % % %
% % % INTRODUCTION% % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % %
\chapter{Introduction}

h 



\clearpage
% % % % % % % % % % % % % % % % % % % % % % % %
% % % LITERATURE OVERVIEW % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %
\chapter{The data}
This chapter is intended to provide an insight into the data. In the following subsection, using exploratory data analysis (EDA), key aspects of the data will be presented. Based on EDA, further considerations are made in the context of the methods to be used.

\section{Exploratory Data Analysis}
The dataset includes 14 female elephants from 14 different herds. Using GPS collars with integrated temperature sensors, the elephants were tracked over a two-year period from 2007 to 2009 in Kruger National Park, South Africa \citep{datasetcite}. GPS temperature sensors were intended to measure the ambient temperature in order to examine movement patterns. For every observation a timestamp including year, month, day, hour and minute has been tracked. 

The dataset contains about 280.000 observations, although they are not uniformly distributed among all animals. Differences in the number of observations results from the fact that some animals were tracked at shorter time periods, e.g. from 2008 to 2009, and therefore have fewer observations. Figure \ref{OpID} shows the number of observations per animal. Each animal has its own ID, reaching from AM91 to AM308. As can be seen, IDs AM91 and AM99 have three times as many observations as IDs AM107 and 306. However, the absolute number of observations per animal is sufficient for the methods applied here and is therefore not a limitation.

Another reason for varying observations per ID results from irregular spaced observations. Most time intervals between two consecutive observations are about half an hour. However, the gap between two observations can be much larger. Even weeks or months can lie in between two consecutive observations. Figure \ref{Irr} represents a distribution of different time intervals in minutes. About 6.000 consecutive observations result in time intervals longer than 150 minutes.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{Obs_per_ID.pdf}
\caption[Observations per animal]{Observations per animal\\
Source: Own representation}
\label{OpID}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{Irregular_data.pdf}
\caption[Time intervals between two consecutive observations (in minutes)]{Time intervals between two consecutive observations (in minutes)\\
Source: Own representation}
\label{Irr}
\end{center}
\end{figure}

This is in fact a drawback in the work from \citet{thaker2019}, since they did not account for this irregular spacing when calculating the speed of the animals, which depends on two consecutive observations. Ignoring this issue leads to biased results when modeling the speed of the animals using models that require equally spaced observations. This is illustrated in figure \ref{Irr2}. Starting from time $t$, the animal travels the distance marked in blue until time $t+1$. The red, dotted line indicates the distance between the two consecutive observations. As can be seen, the indicated distance is much smaller than the travelled distance. Speed is calculated as distance divided by time. If the time interval between two observations gets larger, the calculated speed decreases. As a result, the dependent variable, speed, is biased. 

\begin{figure}[H]
\begin{center}
\includegraphics[width=1\textwidth]{Irregular2.pdf}
\caption[Actual distance traveled vs. calculated distance traveled]{Actual distance traveled vs. calculated distance traveled\\
Source: Own representation}
\label{Irr2}
\end{center}
\end{figure}

When using the methods shown in \citet{thaker2019}, such as generalised additive models (GAMs), generalised additive mixed models (GAMMs) or linear mixed models (LMMs) and ignoring the irregularity in the data, this will result in biased estimators because these methods require equally spaced observations. 

% % % % % % % % % % % % % % % % % % % % % % % %
% % % CONTINUOUS TIME CORRELATED RANDOM WALK  %
% % % % % % % % % % % % % % % % % % % % % % % %

\section{Continuous-time correlated random walk}\label{CTCRW_SEC}
To obtain regular spaced observations, the dataset must be imputed. A flexible method to accomplish this would be a continuous-time correlated random walk (CTCRW). A considerable advantage of a CTCRW is that it allows the modeling of unevenly sampled data without subsampling or interpolation techniques. Moreover, due to an Ornstein-Uhlenbeck process\footnote{which is why the CTCRW is sometimes called velocity Ornstein-Uhlenbeck model}, it can account for autocorrelation in the way that animals are assumed to have a certain kind of inertia, allowing for similar velocities for consecutive times. This is an important feature, because statistically, most tracking data are highly spatially and temporally correlated, which must be taken into account in model formulation \citep{patterson2017statistical}. Since a correlated process does not depend only on the previous observation (which would then be called Markovian) and thus parameter estimation could be difficult, the model is formulated in a state-space framework. A state-space framework then allows the application of the Kalman filter to estimate the parameters via maximum likelihood, with predictions for unobserved location data as a byproduct \citep{Johnson2008}.

To derive the CTCRW, let $\bm{\kappa}(t) = (\kappa_1(t), \kappa_2(t))^\prime$ be the coordinates (longitude and latitude) of an animal location at time $t$. Calculating the difference $\bm{d_{\Delta}(t)} = \bm{\kappa}(t+\Delta) - \bm{\kappa}(t)$ then describes the movement of an animal over $\Delta$ time units. The next step is to formulate this movement as a continuous-time process. If $\Delta \xrightarrow{} 0$ and $\bm{\kappa}(t)$ is a smooth and continuous path, then one obtains the differential equation 
\begin{align}\label{3.0ctcrw1}
d\bm{\kappa}(t) = \bm{v}(t)dt,
\end{align}
with $\bm{v}(t)$ representing the instantaneous velocity \citep{Johnson2008}.

In order to model this instantaneous velocity in continuous-time, an Ornstein-Uhlenbeck process will be used. In particular, for each coordinate axis $c = 1, 2$, the instantaneous velocity, $v_c(t)$, for each time unit $\Delta$, is defined as the autoregressive equation
\begin{align}\label{3.0ctcrw2}
v_c(t + \Delta) = \omega_c + e^{-\rho\Delta}(v_c(t) - \omega_c) + \zeta_c(\Delta).
\end{align}
In \eqref{3.0ctcrw2}, $\omega_c$ is the mean velocity (drift) rate, $\rho$ describes an autocorrelation parameter and $\zeta_c(\Delta)$ represents a normal random variable with distribution 
\begin{align}\label{3.0ctcrw3}
\zeta_c(\Delta)  \sim N\left( 0,\thickspace s^2\frac{(1-e^{-2\rho\Delta})}{2\rho} \right),
\end{align}
with $s$ controlling the overall variability in velocity.\footnote{A strict derivation of the velocity process can be found in \citet{Johnson2008}. In this work, equation \eqref{3.0ctcrw2} already shows the derived velocity process.} Equation \eqref{3.0ctcrw2} states that $v_c(t + \Delta)$ at time $t + \Delta$ equals the mean velocity $\omega_c$, an adjustment for the difference between the velocity at time $t$ and the mean velocity, plus a random variable whose variance increases with $\Delta$, which is stated in equation \eqref{3.0ctcrw3}. The derived process can be used to model the velocity of a moving animal. Figure \ref{OU} shows one hundred simulated Ornstein-Uhlenbeck processes as an example of animal velocity. The figure illustrates the so-called mean-reverting tendency because every process reverts to its mean velocity. However, most of the time, the centre of attraction, $\omega_c$, will be set to zero to indicate that there is no systematic bias in the velocity \citep{Michelot2021}.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{OU.pdf}
\caption[Ornstein-Uhlenbeck process - simulated velocity]{Ornstein-Uhlenbeck process - simulated velocity\\
Source: Own representation}
\label{OU}
\end{center}
\end{figure}

To obtain the location at time $t$ in the continuous-time framework, the bivariate velocity process $\bm{v}(t)$ must be integrated up to time $t$ and added to the initial location $\bm{\kappa}(0)$, which defines the continuous-time location process:
\begin{align}\label{3.0ctcrw4}
\bm{\kappa}(t) = \bm{\kappa}(0) + \int_{0}^{t} \bm{v}(u) \,du.
\end{align}
Thus, the combination of equations \eqref{3.0ctcrw2} and \eqref{3.0ctcrw4} provides the CTCRW model. Basically, equation \eqref{3.0ctcrw2} is an Ornstein-Uhlenbeck process, that is supposed to capture autocorrelation in movement speed and direction, whereas the location process defined in \eqref{3.0ctcrw4} is just the integral of \eqref{3.0ctcrw2}.

However, as mentioned earlier, since the location process relies on consecutive, correlated velocities (i.e., the process is not Markovian), estimating the parameters tends to be difficult. Thus, the CTCRW will be set into a state-space framework \citep{Johnson2008}. The general framework of a Gaussian linear state-space model for a univariate observation comprises of two equations, namely the observation equation and the state equation:
\begin{align}\label{3.0ctcrwssm1}
y_{i} = \bm{G}_i^\prime\bm{a}_i + \varepsilon_i
\\ 
\label{3.0ctcrwssm2} 
\bm{a}_{i+1} = \bm{K}_i\bm{a}_i + \bm{\eta}_i.
\end{align}
In equation \eqref{3.0ctcrwssm1}, $\bm{a}_i$ describes the current state vector, $\varepsilon_i$ is a normal measurement error with variance $H_i$, $\bm{G}_i$ is an appropriately sized transformation matrix for the state vector, and $y_i$ is an observation at time $i$. In equation \eqref{3.0ctcrwssm2}, $\bm{K}_i$ also is an appropriately sized transformation matrix for the state vector. Additionally, $\bm{\eta}_i$ represents normal error vectors with variance-covariance matrix $\bm{Q}_i$ \citep{Johnson2008}.

In the following, the CTCRW is reformulated into equations \eqref{3.0ctcrwssm1} and \eqref{3.0ctcrwssm2} to obtain a state-space model version. Assuming that the locations $\bm{y}_i = (y_{1i}, y_{2i})^\prime$ are observed at times $t_1, \mathellipsis, t_n$ and, conditioning on the true location $\bm{\kappa}(t) = (\kappa_1(t), \kappa_2(t))^\prime$, this will yield the observation equation for animal movement:
\begin{align}\label{3.0ctcrw5}
y_{ci} = \kappa_{ci} + \varepsilon_{ci},\thickspace\thickspace\thickspace\thickspace\varepsilon \sim N(0, \thickspace H_{ci}).
\end{align}
The variance $H_{ci}$ of the error term $\varepsilon_{ci}$ in equation \eqref{3.0ctcrw5} could depend on external location quality covariates (for GPS data, the measurement error often is sufficiently small so that it will be ignored).

In addition, taking into account the velocity process described in \eqref{3.0ctcrw2} and the formulation of the location process given in equation \eqref{3.0ctcrw4}, the state $\bm{a}_i$ can be obtained by bundling the velocity process with the location process in a single state vector, since the velocity process itself is Markovian \citep{Johnson2008}.

The true location equation (which corresponds to the state-space model's state equation) is somewhat more difficult to formulate since $\bm{\kappa}(t)$ is not Markovian. However, taking into account the velocity process described in \eqref{3.0ctcrw2} and the formulation of the location process given in equation \eqref{3.0ctcrw4}, the state $\bm{a}_i$ can be obtained by bundling the velocity process to the location process into a single state vector, since the velocity process itself is Markovian. Therefore, this yields the state equation 
\begin{align}\label{3.0ctcrw6}
\kappa_{c,i+1} = \kappa_{ci} + v_{ci}\left(\frac{1-e^{-\rho\Delta_i}}{\rho}\right) + \vartheta_{ci}.
\end{align}
As before, $\Delta_i$ is the difference between two consecutive times and $\vartheta_{ci}$ represents normal error vectors (this is the first entry of $\bm{\eta}_i$ in the general formulation) with variance
\begin{align}\label{3.0ctcrw7}
\text{Var}(\vartheta_{ci}) = \frac{s^2}{\rho^2}\left(\Delta_i - \frac{2}{\rho}(1- e^{-\rho\Delta_i})+ \frac{1}{2\rho}(1- e^{-2\rho\Delta_i})\right).
\end{align}
Furthermore, with the covariance 
\begin{align}\label{3.0ctcrw8}
\text{Cov}(\zeta_{ci}, \vartheta_{ci}) = \frac{s^2}{2\rho^2}\left(1 - 2e^{-\rho\Delta_i} + e^{-2\rho\Delta_i}\right)
\end{align}
between $\zeta_{ci}$ and $\vartheta_{ci}$ (consequently, $\zeta_{ci}$ is the second entry of $\bm{\eta}_i$ in the general formulation), the variance-covariance matrix of $\bm{\eta}_{ci}$ can be specified as 
\begin{align}\label{3.0ctcrw9}
\bm{Q}_{ci} = \begin{pmatrix}
   \text{Var}(\vartheta_{ci}) & \text{Cov}(\zeta_{ci}, \vartheta_{ci}) \\
   \text{Cov}(\zeta_{ci}, \vartheta_{ci}) & \text{Var}(\zeta_{ci})
   \end{pmatrix}.
\end{align}
As a last step, defining $\bm{G}_i = (1\thickspace 0)^\prime$, $\bm{a}_i = (\kappa_{ci}\thickspace v_{ci})^\prime$, $\bm{\eta}_{ci} = (\vartheta_{ci}\thickspace \zeta_{ci})^\prime$ and $H_{ci} = H(l_i)$, where $l_i$ is a known location quality covariate, and in combination with the matrix
\begin{align}\label{3.0ctcrw10}
\bm{K}_{i} = \begin{pmatrix}
   1 & (1- e^{-\rho\Delta_i})/\rho \\
   0 & e^{-\rho\Delta_i}
   \end{pmatrix},
\end{align}
all components of a basic state-space formulation are specified. In particular, with equations \eqref{3.0ctcrw2}, \eqref{3.0ctcrw5}, and \eqref{3.0ctcrw6}, one obtains a CTCRW model in a state-space framework \citep{Johnson2008}. 

This reformulation of the CTCRW allows the use of a Kalman filter to estimate the movement parameters $\hat{\theta} = (\hat{\rho}_1, \hat{\rho}_2, \hat{s}_1, \hat{s}_2)^\prime$ via maximum likelihood estimation. As mentioned earlier, the byproduct of estimation using a Kalman filter are (partly) unobserved, equally spaced locations $\bm{\hat{\kappa}}(t)$ (and velocities $\bm{\hat{v}}(t)$) \citep{Johnson2008}.

Using the CTCRW, the dataset at hand has been imputed. The selected time interval is one hour, as this ensures sufficient granularity and at the same time a good prediction quality of the CTCRW. Figure \ref{PathsCTCRW} graphically represents the imputed hourly paths of all 14 animals. The black dots represent the observed locations and the colored paths correspond to the predicted movement path obtained by the CTCRW. Based on the regular movement path, the variables 'speed', 'step length' and 'turning angle' were computed. The CTCRW was coded in the statistical software R using the functions $\mathtt{crwFit}$ and $\mathtt{crwPredict}$ from the $\mathtt{crawl}$ package \citep{johnson2018package}. The computation of the step length and the turning angle was accomplished using the function $\mathtt{prepData}$ via the package $\mathtt{moveHMM}$ \citep{michelot2016movehmm}. Step lengths are defined as the distance between two consecutive observations, whereas turning angles corresponds to the changes of direction between two consecutive observations \citep{patterson2017statistical}. The computation time took only a few minutes on a 4.9 GHz Intel Core i7 9700k and 16GB DDR4 RAM.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{All_Imputed_Paths-compressed.pdf}
\caption[Imputed Paths]{Imputed Paths\\
Source: Own representation}
\label{PathsCTCRW}
\end{center}
\end{figure}

However, as already mentioned, there are also time intervals of several months between two consecutive observations. This temporal discrepancy could not be modelled satisfactorily by a CTCRW. Temporal discrepancies that are too large to model do not affect all time series, but only a few animals. This is exemplified by animal 'AM107'. Figure \ref{AM107CTCRW} represents a plot of the imputed step lengths of AM107 depending on the tracking time.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{AM107_FAIL.pdf}
\caption[AM107 modelled step lengths using CTCRW]{AM107 modelled step lengths using CTCRW\\
Source: Own representation}
\label{AM107CTCRW}
\end{center}
\end{figure}

The diagram shows that there are time intervals with a seemingly constant step length over weeks or even months. The (almost) constant step length results from the fact that the CTCRW can not satisfactorily model hourly time intervals because the temporal discrepancy between two consecutive observations in time is too large and results in infinitesimal changes in the hourly step length. As mentioned, this is the case for just a few animals. Instead of completely ignoring the animal's movement path, a sequence is used where there are no weeks or months between two consecutive observations. As an example, the time series of 'AM107' was truncated to August 2007 to February 2008.

A CTCRW thus provides a new dataset with additional, useful information and movement paths of all 14 animals, which forms the basis for the methods that will be applied. In particular, the variables 'speed', 'step length' and 'turning angle' can now be modelled using methods that require equally spaced data in order to make statistical inference about the movement pattern of elephants.

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % % METHODS  % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % % 

\chapter{Methods}
This chapter is intended to provide a theoretical foundation for the concepts that will be applied in the next chapter. In particular, fixed and random effects in regression models are described, as well as an additive extension of the family of generalised linear models called GAMs. The last two sections of this chapter introduce two concepts considered as flexible tools for handling time series data: Hidden Markov models (HMMs) and stochastic differential equations (SDEs). HMMs will be covered first. Then SDEs are slightly extended to a time-varying version of common stochastic processes.


% % % % % % % % % % % % % % % % % % % % % % % %
% % % Mixed Models and GAMs % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %


\section{Mixed Models and Generalised Additive Models}\label{GAMz_theory}
With the CTCRW model from the previous section, the data are now available at regular intervals expanding the possibilities of methods to be used. Over the past three decades, numerous extensions of the standard linear regression model have been developed. In the work of \citet{thaker2019}, LMMs, GAMs, and GAMMs were used. This section focuses on a discussion about the 'mixed' part of these model formulations as well as GAMs.
\subsection*{Mixed Models}
Mixed models are, in their simplest form, an extension of the linear model
\begin{align}\label{3.0mm1}
\bm{y} = \bm{X}\bm{\xi} + \bm{\varepsilon},\thickspace\thickspace\thickspace\thickspace\bm{\varepsilon} \in \text{N}(\bm{0}, \textbf{I}\sigma^2) 
\end{align}
to
\begin{align}\label{3.0mm2}
\bm{y} = \bm{X}\bm{\xi} + \bm{Z}\bm{b} + \bm{\varepsilon},\thickspace\thickspace\thickspace\thickspace\bm{b} \in \text{N}(\bm{0}, \bm{\Psi}),\thickspace\thickspace\thickspace\thickspace \bm{\varepsilon} \in \text{N}(\bm{0}, \bm{\Lambda}\sigma^2),
\end{align}
with $\bm{b}$ as a vector containing so-called random effects with mean zero and covariance matrix $\bm{\Psi}$ \citep{Wood2017}. Random effects can model heterogeneity in the data, particularly if the data set has a hierarchical structure and many observations are available for several observational units (different animals for example). For example, if one animal has traveled a bigger distance in a month than others, this does not mean that it will consistently travel farther than other animals. That would rather be a random effect than a particular subject's effect. The first terms of \eqref{3.0mm1} and \eqref{3.0mm2} simply represent the regressor matrix and the corresponding coefficients $\bm{\xi}$. The matrix $\bm{Z}$ is a simple model matrix for the random effects. In particular, $\bm{Z}$ contains fixed coefficients that describe how $\bm{y}$ depends on the random effects. Usually, $\bm{Z}$ is a subset of $\bm{X}$ because it does not make sense to assume a random deviation for every covariate effect in most applications. Moreover, the matrix $\bm{\Lambda}$ is a positive definite matrix that can be used to model autocorrelation. Consequently the elements of the response vector, $\bm{y}$, may no longer be assumed to be independent \citep{Wood2017}. However, for most regression models used in the literature, $\bm{\Lambda}$ corresponds to the identity matrix $\textbf{I}$. 
\subsection*{Generalised Additive Models}
GAMs are a generalisation of additive models. The linear predictor of a GAM contains one or more smoothing functions for the expected value of the response, while the response variable itself may follow any exponential family distribution: 
\begin{align}\label{3.0gam1}
h(\mathbb{E}(y_i)) = \bm{X}_i^*\bm{\xi} + g_1(x_{1i}) + g_2(x_{2i}) + g_3(x_{3i}) + \cdots.
\end{align}
In equation \eqref{3.0gam1}, $\bm{X}_i^*$ is the $i$-th row of a model matrix that corresponds to any strictly parametric model component, with parameter vector $\bm{\xi}$. The expression $g_j(x_j)$ is a smoothing function of some covariate $x_j$. The response variable $y$ follows any exponential family distribution, and $h$ is a known, monotonic, twice differentiable link function \citep{Wood2017,hastie1990generalized}. The concept of link functions is also known from generalised linear models. \footnote{For a detailed discussion of link functions \citep{McCulloch2008}.}

According to \citet{Wood2017}, the smoothing function $g_j(\cdot)$ can be written as a linear combination of $d_j$ basis functions ${{\psi_{jk}}}$ with parameters $\beta_{jk}$ with
\begin{align}\label{3.gam2}
    g_j(x) = \sum_{k = 1}^{d_j} \beta_{jk}{\psi_{jk}}(x),\thickspace\thickspace\thickspace\thickspace j = 1, \mathellipsis, J.
\end{align}
Equation \eqref{3.gam2} states that $g_j(x)$ is an element of the defined basis. The basis functions themselves are supposed to represent unknown functions. Consequently, it makes sense to choose those basis functions, that are suitable for approximating known functions. An appropriate option are splines, for example (cyclic) cubic regression splines, P-splines and thin plate regression splines could be used. In fact, most smoothing functions are spline based.

For model fitting, the parameters xi and beta must be estimated, however, for convenience, it will be assumed that there is no parametric model component and thus, xi is neglected in the following. The estimation process is called penalised likelihood maximisation because ordinary likelihood maximisation is prone to overfitting. The penalties are defined so that $g_j$ is not overly wiggly, which is intended to account for the bias-variance trade-off.\footnote{The estimation methods are discussed in more detail in \citet{Wood2017}.} 
To give an example of a spline based smoother, P-splines are used. In case there only is one smoothing function, the objective function
\begin{align}\label{3.gam3}
    \Vert\bm{y} - \bm{X\beta}\Vert^2 + \lambda \int_{0}^{1} [g^{\prime\prime}(x)]^2 \,dx
\end{align}
has to be minimised, where the first term represents the sum of squared residuals, and the second term is a measure for the overall curvature, which is weighted by a smoothing parameter $\lambda$ \citep{Wood2017}. It basically means that $\lambda$ controls the trade-off between under- and overfitting. For $\lambda \xrightarrow{} \infty$, the curvature of the smoothing function $g$ will be as low as possible- resulting in a straight line. On the other hand, $\lambda = 0$ means that the curvature will not be penalised at all. Since $g(x)$ is linear in the parameters, $\beta_{k}$, the penalisation component can be written as 
\begin{align}\label{3.gam3_sonder}
    \int_{0}^{1} [g^{\prime\prime}(x)]^2 \,dx = \bm{\beta^TS\beta},
\end{align}
where $\bm{S}$ is a matrix of known coefficients, turning \eqref{3.gam3} into
\begin{align}\label{3.gam3_updated}
    \Vert\bm{y} - \bm{X\beta}\Vert^2 + \lambda \bm{\beta^TS\beta}.
\end{align}
The objective function \eqref{3.gam3} then needs to be minimised w.r.t. $\bm{\beta}$, given $\lambda$. In practice, this is often achieved by using penalised iterative re-weighted least squares (PIRLS). The penalisation parameter $\lambda$ can be estimated by cross validation or maximum likelihood estimation. In favor of computational efficiency, generalised cross validation (GCV) as well as the so called restricted maximum likelihood estimation (REML) are often used in practice to estimate $\lambda$ \citep{wood2004stable}. REML is used instead of ordinary maximum likelihood estimation, because ordinary maximum likelihood estimation underestimates variance components in this context \citep{Wood2017}.\footnote{For large sample sizes, the bias of the variance component gets smaller. However, when working with high dimensional data, the variance estimators can be seriously biased.} Figure \ref{GAM} graphically represents how P-splines work. An amount of $d$ basis functions will be weighted by the estimated parameters $\bm{\beta}$ and then form a regression function. 

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{SPLINE_BASIS.pdf}
\caption[P-spline fitted regression function]{P-spline fitted regression function\\
Source: Own representation based on Langrock (2015).}
\label{GAM}
\end{center}
\end{figure}

Turning to model selection, AIC is a popular choice. In the context of GAM(M)s, this criterion requires some further attention. There are two main approaches for the AIC: The marginal AIC and the conditional AIC. Basically they differ in the way they are treating smooths. In order to obtain the marginal AIC, the marginal likelihood of the model is used. Moreover, smooths are treated as random effects and will not count for the number of parameters used. As being mentioned, the marginal likelihood tends to underestimate variance components. However, replacing ordinary likelihood estimation by REML does not solve the issue, because the restricted likelihood of a model is only comparable to those models with the same fixed effects structure and hence cannot be compared in the sense of smooths (since the smooths are treated as random effects). Consequently, it cannot be evaluated, if a smooth should be added to the model or not. In contrast, the conditional AIC is based on the likelihood of all coefficients and uses the effective degrees of freedom (EDF). Moreover, smooths are treated as fixed effects. As \citet{greven2010behaviour} have shown, the conditional version of the AIC tends to select models that are rather complex, especially when the model contains random effects. Thus, \citet{wood2016smoothing} proposed a correction to the EDF to obtain a corrected version of the AIC. In the rest of this work, the corrected version of the AIC will be used for GAM(M)s.

All in all, GAMs provide a very accessible framework to model all kinds of relationships due to flexible smoothing functions. However, when modeling the observations in a time series context and assuming the observations are not independent of each other, the implementation of autocorrelations can be very time-consuming and computationally demanding. Moreover, in movement ecology, there are many approaches to interpret the movement characteristics of animals by movement modes (see \citet{patterson2009classifying}, for example). Thus, a popular method of modeling animal tracking data will be discussed in the next section: HMMs.

% % % % % % % % % % % % % % % % % % % % % % % %
% % % HIDDEN MARKOV MODELS  % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %

\section{Hidden Markov Models}
This subsection deals with HMMs. Once the dataset has been imputed, HMMs are flexible tools for dealing with time series data, especially for discrete-valued series. The main reason is that HMMs provide a convenient way to relax the assumption that the observed data are independent (as will be seen, they are conditionally independent) without being overly complex. HMMs can be conceptually divided into two blocks: the first block is a set of distributions that depend on the second block, an unobserved Markov chain consisting of different states. Depending on the active state, a particular distribution is chosen. That is, the state-dependent distribution then generates an observation $Y_t$ for each $t = 1,\mathellipsis, T$. This section has been divided into three logical subsections to make it more concise and clear. 
\subsection*{Markov Chains}

Since the second block, the so-called Markov chain, is essential for understanding an HMM, a brief introduction is given. In general, a Markov chain is a sequence of discrete random variables $\{C_t : t \in \mathbb{N}\}$ if it satisfies the Markov property:
\begin{align}\label{3.0mc}
Pr(C_{t+1} \mid C_t,\mathellipsis, C_1) = Pr(C_{t+1} \mid C_t).
\end{align}
This property induces a certain dependency structure: given the past $C_{t-1},\mathellipsis, C_1$ of the process, the future value $C_{t+1}$ only depends on $C_t$. Another major aspect of Markov chains is that they can take on $m$ different values (states) in some countable set $S$ with transition probabilities
\begin{align}\label{3.0mc1}
\gamma_{ij}(t, s) = Pr(C_{s+t} = j \mid C_s = i).
\end{align}
Equation \eqref{3.0mc1} states that the probability of being in state $j$ at time $s + t$, given the chain is in state $i$ at time $s$, is denoted by $\gamma_{ij}(t, s)$. If the transition probability does not depend on $s$, the Markov chain is considered homogeneous \citep{Zucchini2016}. This will be denoted by $\gamma_{ij}(t)$ and shall be assumed in this work unless there is an explicit indication of the contrary. The row stochastic matrix $\bm{\Gamma}(1)$, which will be abbreviated as $\bm{\Gamma}$ contains the one-step transition probabilities and is given by
\begin{align}
    \bm{\Gamma} = \begin{pmatrix}
   \gamma_{11} & \cdots & \gamma_{1m} \\
   \vdots & \ddots & \vdots \\
   \gamma_{m1} & \cdots & \gamma_{mm}
   \end{pmatrix}. 
\end{align} 
Figure \ref{M_St} provides a graphical representation of a basic 3-state Markov process. The arrows indicate the probabilities of switching from state $i$ to state $j$. The matrix $\bm{\Gamma}(1)$ contains all one-step transition probabilities.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{MARKOV_STATES.pdf}
\caption[3-state Markov Process]{3-state Markov Process \\
Source: Own representation based on \citet{Zucchini2016}.}
\label{M_St}
\end{center}
\end{figure}

In most cases, one is interested in the long-term behaviour of the Markov chain and if there exists a limiting distribution of the chain. Such a limiting distribution is closely related to the existence of a so-called stationary distribution.
A Markov chain is said to have a stationary distribution $\bm{\delta}$, where $\bm{\delta}$ is a row vector with entries $\delta_j$, if
\begin{align}\label{3.pr1}
    & \delta_j \geq 0,\thickspace\thickspace\thickspace\thickspace\forall j \\
    &\bm{\delta}\bm{\Gamma} = \bm{\delta} \label{3.pr2}\\
    &\bm{\delta}\mathbf{1}^{\prime} = 1. \label{3.pr3}
\end{align}
Equation \eqref{3.pr2} indicates the stationarity while \eqref{3.pr1} and \eqref{3.pr3} specify that $\bm{\delta}$ is a probability distribution since the entries add up to $1$ and are non-negative. If one additionally assumes that the Markov chain is irreducible and aperiodic, it follows that a unique limiting distribution exists, which is the stationarity distribution \citep{Zucchini2016}. The vector $\bm{\delta}$ is a stationary distribution if and only if 
\begin{align}\label{3.024mc}
    \bm{\delta}(\bm{I}_m - \bm{\Gamma} + \bm{U}) = \mathbf{1},
\end{align}
where $\bm{I}_m$ is the $m \times m$ identity matrix, and $\bm{U}$ is the $m \times m$ matrix of ones. Thus, equation \eqref{3.024mc} can be used to compute the stationary distribution if the assumptions hold \citep{Zucchini2016}. For the tracking data, it can be assumed that the underlying process is in its stationary distribution since the elephants existed before they have been tracked, and thus the process has been running for some time.

The properties of Markov chains form the core of the model structure of HMMs. The purpose of Markov chains in this context is that they can conveniently relax the independence assumption of the observations. In particular, they allow the observations to be independent conditional on the state being active, as shall be seen in the mathematical definition of HMMs.
\subsection*{Model formulation}

Formally HMMs consist of an unobserved parameter process $\{C_t : t \in \mathbb{N}\}$ and an observed state-dependent process $\{Y_t : t \in \mathbb{N}\}$ with state-dependent distributions. 
In particular, the underlying parameter process is a Markov chain and consists of $m$ states. Most HMMs satisfy the Markov property, which is given in equation \eqref{3.0mc}. This allows for serial dependence in the observations, such that a simple HMM can be characterised by
\begin{align}\label{3.000}
Pr(C_t \mid \mathbf{C}^{(t-1)}) = Pr(C_t \mid C_{t-1}),\thickspace\thickspace\thickspace\thickspace t = 2, 3,\mathellipsis
\end{align}
\begin{align}\label{3.01}
Pr(Y_t \mid \mathbf{Y}^{(t-1)}, \mathbf{C}^{(t)}) = Pr(Y_t \mid C_t),\thickspace\thickspace\thickspace\thickspace t \in \mathbb{N}.
\end{align}
The above equations \eqref{3.000} and \eqref{3.01} show that conditioning on the history of the Markov chain $\mathbf{C}^{(t-1)}$, $C_{t}$ only depends on $C_{t-1}$. Moreover, the Markov property implies that $\{C_t : t \in \mathbb{N}\}$ is fully characterised by the stationary distribution $\bm{\delta}$ and the transition probability matrix $\bm{\Gamma}$. The same principle applies to the state-dependent process, namely that the distribution of $Y_t$ depends only on $C_t$ and neither on a previous state $\mathbf{C}^{(t-1)}$ nor on the history of observations $\mathbf{Y}^{(t-1)}$. Thus, equation \eqref{3.01} defines the said conditional independence assumption. In particular, equation \eqref{3.01} implies that $\{Y_t : t \in \mathbb{N}\}$ is fully characterised by the state-dependent distributions \citep{Zucchini2016}. Figure \ref{HMM_BASIC} illustrates the structure of a basic HMM. The unobserved parameter process $C_{t}$ and the state-dependent process $Y_{t}$ are shown.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{HMM_BASIC_HMM.pdf}
\caption[Basic HMM structure]{Basic HMM structure\\
Source: Own representation based on \citet{Zucchini2016}.}
\label{HMM_BASIC}
\end{center}
\end{figure}

The state-dependent distributions in the case of discrete observations can be written as \begin{align}\label{3.02}
p_i(y) = Pr(Y_t = y \mid C_t = i).
\end{align}
Equation \eqref{3.02} indicates that $p_i$ is the probability mass function of $Y_t$, when the Markov chain is in state $i$ at time $t$ \citep{Zucchini2016}. It is worth noting that the state-dependent distributions are usually chosen as a class of parametric distributions. However, sometimes it may be challenging to decide which parametric distribution to use. An alternative is to estimate a nonparametric state-dependent distribution, for example, by using P-splines as shown in \citet{Langrock2015}. Figure \ref{HMM_CHOICE} shows the process that generates the observations in a two-state HMM. Each state corresponds to a state-dependent distribution, and the active distribution then generates the observation.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{HMM_STATE_CHOICE.pdf}
\caption[2-state HMM process]{2-state HMM process\\
Source: Own representation based on \citet{Zucchini2016}.}
\label{HMM_CHOICE}
\end{center}
\end{figure}

With the state-dependent distributions one can obtain the marginal distribution of $Y_t$. By defining $u_i(t) = Pr(C_t = i)$ for $t = 1,\mathellipsis, T$ it follows that
\begin{align}\label{3.03}
    Pr(Y_t = y)  & =  \sum_{i=1}^{m}Pr(C_t = i)Pr(Y_t = y \mid C_t = i) \\
    & = \sum_{i=1}^{m}u_i(t)p_i(y), \label{3.0301}
\end{align}
or in matrix notation:
\begin{align}\label{3.04}
    Pr(Y_t = y)  & =  (u_1(t),\mathellipsis, u_m(t))\begin{pmatrix}
   p_1(y) & & 0 \\
   & \ddots & \\
   0 & & p_m(y)
   \end{pmatrix}
   \begin{pmatrix}
   1 \\
   \vdots \\
   1
   \end{pmatrix}\\
    & = \mathbf{u}(t)\mathbf{P}(y)\mathbf{1}^{\prime}, \label{3.05}
\end{align}
where $\mathbf{P}(y)$ is a diagonal matrix of all state-dependent distributions \citep{Zucchini2016}. Equations \eqref{3.0301} and \eqref{3.05} show that the marginal distribution of an HMM is simply a mixture distribution. If the underlying Markov chain is stationary with stationary distribution $\bm{\delta}$, equation \eqref{3.05} even reduces to
\begin{align}\label{3.zsmf}
    Pr(Y_t = y) = \bm{\delta}\mathbf{P}(y)\mathbf{1}^{\prime}.
\end{align}
The discussed framework above forms the basis for all HMMs. Thus, in its simplest form, an HMM is a dependent mixture model where a Markov chain selects the state-dependent distributions. On this basis, there are numerous extensions of HMMs.\footnote{And in fact, an HMM is itself a special case of state-space models (SSMs). SSMs are also doubly stochastic processes, with the state process being continuous-valued, whereas HMMs take on a finite number of discrete states.} Some common examples are multivariate time series (assuming longitudinal and contemporaneous conditional independence), integration of covariates and seasonality, random effects, and higher-order Markov chains. As will be seen, some of these are used in later sections, particularly the multivariate case and the integration of covariates and seasonality. First, before an HMM can be fitted to data, the likelihood must be calculated.
\subsection*{Likelihood}

The likelihood can be used to estimate the parameters linked to an HMM (e.g. the parameters of the assumed state-dependent distributions and transition probabilities). However, before the likelihood can be maximised, it must be calculated.
The likelihood $L_T$ of an HMM for consecutive observations $y_1,\mathellipsis, y_T$ is given by the general formula
\begin{align}\label{3.lklhd}
    L_T = \bm{\delta}\mathbf{P}(y_1)\bm{\Gamma}\mathbf{P}(y_2)\cdots\bm{\Gamma}\mathbf{P}(y_T)\mathbf{1}^{\prime}.
\end{align}
Equation \eqref{3.lklhd} shows that the initial distribution $\bm{\delta}$ of the Markov chain is multiplied by the diagonal matrix of the state-dependent distributions evaluated at $y_1$ and so forth \citep{Zucchini2016}. 

It has been (and still is) claimed in several statements in the literature that direct maximisation of the likelihood expressed in \eqref{3.lklhd} is not feasible because $m^T$ summands need to be computed. In terms of complexity, this leads to $O(Tm^T)$ operations and motivates complex estimation techniques such as the Expectation-Maximisation (EM) algorithm or Markov chain Monte Carlo (MCMC)-based methods. However, from more general discussions, it is well known that recursive schemes can be used to compute the likelihood in a computationally much more efficient way. The likelihood described in equation \eqref{3.lklhd} clearly shows such a recursive scheme. In fact, defining a vector $\bm{\alpha}_t$ for $t = 1,\mathellipsis, T$ leads to the so-called forward probabilities:
\begin{align}\label{3.fwds}
    \bm{\alpha}_t = \bm{\delta}\mathbf{P}(y_1)\bm{\Gamma}\mathbf{P}(y_2)\cdots\bm{\Gamma}\mathbf{P}(y_T) = \bm{\delta}\mathbf{P}(y_1) \prod_{s = 2}^{t} \bm{\Gamma}\mathbf{P}(y_s).
\end{align}
The forward probabilities defined in \eqref{3.fwds} contain information on the likelihood up to time $t$. In particular, the likelihood $L_T$ can be computed using the forward algorithm:
\begin{align}\label{3.fwdalg1}
    & \bm{\alpha}_1 =  \bm{\delta}\mathbf{P}(y_1); \\
    & \bm{\alpha}_t = \bm{\alpha}_{t-1}\bm{\Gamma}\mathbf{P}(y_t)\thickspace\thickspace\thickspace\thickspace \text{for }t = 2, 3,\mathellipsis, T; \label{3.fwdalg2}\\
    & L_T =\bm{\alpha}_T\mathbf{1}^{\prime}. \label{3.fwdalg3}
\end{align}
Using the forward algorithm shown above, $\bm{\alpha}_1$ can be computed first, then $\bm{\alpha}_2$ based on $\bm{\alpha}_1$ and so forth. As expressed in \eqref{3.fwdalg3}, with $\bm{\alpha}_T = (\alpha_T(1),\mathellipsis, \alpha_T(m))$ the likelihood then is $\sum_{j = 1}^{m} \alpha_T(m) = L_T$ \citep{Zucchini2016}.

By taking advantage of the forward algorithm, the number of operations involved is of order $Tm^2$ ($O(Tm^2)$, is just a fractal of $O(Tm^T)$) and thus can easily be maximised numerically in order to tailor an HMM to data. The purpose of presenting the computation of the likelihood is to show that HMMs do not require substantially higher computational effort compared to the other methods discussed in this text. However, there are some issues to consider when performing direct numerical maximisation, such as numerical underflow and constrained parameters, but this is beyond the scope of this text \citep{Zucchini2016}. 

As mentioned at the start of this subsection, HMMs provide a flexible tool for time series analysis, especially in the discrete-time context. However, the HMMs described above as well as GAMs and GAMMs can only handle datasets with regularly spaced observation times. Since the given dataset has irregularly spaced observation times, the data need to be imputed before the mentioned concepts can be tailored to the data. Given this circumstance, an alternative approach uses SDEs, as shown in the following subsection.

% % % % % % % % % % % % % % % % % % % % % % % %
% % % STOCHASTIC DIFFERENTIAL EQUATIONS % % % %
% % % % % % % % % % % % % % % % % % % % % % % %

\section{Stochastic Differential Equations}\label{SDE_SEC}

In addition to the HMM framework, SDEs are also a popular choice for modeling time series data. An essential aspect of this is that SDEs do not depend on regular spaced observation times, are relatively simple, and have parameters that are easy to interpret. Consequently, no imputation is needed. However, SDEs are formulated in continuous-time, whereas HMMs are based on a discrete-time framework. Because SDEs are considered to be not flexible enough to model animal movement, this method will be extended in the following. 

Formally the most common SDE is stated as
\begin{align}\label{3.sdebasic1}
    dY_t = \mu(Y_t, t)dt + \sigma(Y_t, t)dW_t,\thickspace\thickspace\thickspace\thickspace Y_0 = y_0,
\end{align}
with $W_t$ being a Wiener process and $y_0$ as the initial condition of the stochastic process $Y_t$. Equation \eqref{3.sdebasic1} represents the evolution of a process $Y_t$ with $\mu$ as the expected change in the process for an infinitesimal time interval. The function $\sigma$, frequently referred to as diffusion, is a measure for the variability \citep{Michelot2021}. A solution to \eqref{3.sdebasic1} often results in parametric functions for $\mu$ and $\sigma$. 

A more recent approach by \citet{Michelot2021} provides more flexibility within the structure of the basic SDE shown in \eqref{3.sdebasic1} and allows for broad time-varying dynamics. In particular, $\mu$ and $\sigma$ can be specified as basic penalty smoothing splines. Splines have already been discussed in the section dealing with GAMs. In principle, the approach by \citet{Michelot2021} allows the integration of linear and nonlinear effects in covariates and random effects in SDEs. Unlike HMMs, where the estimated parameters of the assumed distributions are piece-wise constant (for each state), this approach allows smooth, time-varying parameters \citep{Michelot2021}.
\subsection*{Model formulation}

In this setting, a stochastic process $Y_t$ is defined by 
\begin{align}\label{3.sdebasic2}
    dY_t = \mu(Y_t, \bm{\theta}_t)dt + \sigma(Y_t, \bm{\theta}_t)dW_t,\thickspace\thickspace\thickspace\thickspace Y_0 = y_0.
\end{align}
In contrast to \eqref{3.sdebasic1}, equation \eqref{3.sdebasic2} describes a dependency of $\mu$ and $\sigma$ from a time-varying parameter vector $\bm{\theta}_t$, whereas $\mu$ and $\sigma$ itself determine the type of SDE (e.g. (geometric) Brownian Motion, Ornstein-Uhlenbeck process, CTCRW, etc.).

The parameter vector $\bm{\theta}_t$ itself depends on $J$ covariates $x_{1t},\mathellipsis, x_{Jt}$ and consists of components
\begin{align}\label{3.sdebasic3}
    h(\theta_t) = \beta_0 + g_1(x_{1t}) + \cdots, + g_J(x_{Jt}),
\end{align}
with a link function $h$ and $\beta_0$ as an intercept parameter. The functions $g_j(\cdot)$ are not further specified as they could be linear or nonlinear effects of a covariate as well as random effects or smooth functions \citep{Michelot2021}. In case the functions $g_j(\cdot)$ are smoothing functions, the definition of smoothing functions in \eqref{3.gam2} can be applied. Equations \eqref{3.sdebasic2}, \eqref{3.sdebasic3}, and in combination with \eqref{3.gam2}, form a model which is called varying-coefficient stochastic differential equation, which itself is based on varying-coefficient models described by \citet{hastie1993} in the context of GAMs.
\subsection*{Likelihood}

In order to fit this model formulation to data, the likelihood must be computed. In particular, the aim is to estimate the relationship between the time-varying parameter vector $\bm{\theta}_t$, which determines the drift $\mu$, and the diffusion $\sigma$ of the stochastic process $Y_t$, as can be seen in equation \eqref{3.sdebasic2}.

Given a sequence of $n$ consecutive observations (with potentially irregular spaced observation times) $y_1,\mathellipsis, y_n$ from the process $Y_t$ with times $t_1,\mathellipsis, t_n$ the goal is to maximise the likelihood $L$ given the data with parameter vector $\bm{\theta}_t$:
\begin{align}\label{3.sdebasic5}
    L(\bm{\xi}, \bm{\beta} \mid  y_1,\mathellipsis, y_n) = p(Y_{t_1} = y_1)\prod_{i = 1}^{n-1}p(Y_{t_{i+1}} = y_{i+1} \mid Y_{t_i} = y_i).
\end{align}
There are some aspects to note in equation \eqref{3.sdebasic5}. First, stochastic processes are Markovian, leading to the same dependency structure imposed by equation \eqref{3.0mc}. The dependency structure implies that the likelihood of $n$ consecutive observations can be obtained as the product of the likelihoods of the individual transitions \citep{Michelot2021}. The vector $\bm{\beta}$ contains all coefficients of equation \eqref{3.gam2} (e.g., the parameters for the smooth or random effects functions $g_j(x)$) and the vector $\bm{\xi}$ respectively, contains all other coefficients of the model (that is, $\bm{\xi}$ contains the parameters for linear covariate effects) with $p(\cdot)$ as the probability density function (pdf). 

For evaluating the likelihood it is assumed that $y_1$ is deterministic, i.e. $p(Y_{t_1} = y_1) = 1$. It follows that only the conditional densities $p(Y_{t_{i+1}} \mid Y_{t_i})$ need to be evaluated. In this text, two processes are used, namely Brownian motion and Ornstein-Uhlenbeck processes (and CTCRW, as it is based on Ornstein-Uhlenbeck processes). There are closed-form expressions for the densities of these two processes. However, for the general case, the pdf $p(\cdot)$ can be approximated by a pdf of a normal distribution using the Euler-Maruyama discretization.

As mentioned before, $\mu$ and $\sigma$ determine which type of SDE is considered. For time-varying Brownian motion and time-varying Ornstein-Uhlenbeck processes, $\mu$ and $\sigma$ each depend only on one time-varying parameter, leading to $\bm{\theta}_t = (\theta_t^{(1)}, \theta_t^{(2)})$ \citep{Michelot2021}. In the following, $\theta_t^{(1)} = r_t$ and $\theta_t^{(2)} = s_t$ will be used for simplicity.

In fact, defining equation \eqref{3.sdebasic2} as a time-varying version of Brownian motion can be achieved by setting $\mu(Y_t, \bm{\theta}_t) = r_t$ and $\sigma(Y_t, \bm{\theta}_t) = s_t$ which yields
\begin{align}\label{3.bm1}
    dY_t = r_tdt + s_tdW_t,\thickspace\thickspace\thickspace\thickspace Y_0 = y_0.
\end{align}
In particular, using the Euler-Maruyama discretization for time-varying Brownian motion processes, one can obtain the approximated density
\begin{align}\label{3.euler-maruyama1}
    p(Y_{t + \Delta} = y_{t + \Delta} \mid Y_t = y_t) = \phi(y_{t+\Delta};\thickspace y_t + r_t\Delta, s_t^2\Delta),
\end{align}
with $\phi(y; \thickspace b, k)$ being the pdf of a normal distribution with mean $b$ and variance $k$. The approximated density described in equation \eqref{3.euler-maruyama1} can now be substituted into equation \eqref{3.sdebasic5} which yields the approximate likelihood for the time-varying Brownian motion process. 

The same is true for Ornstein-Uhlenbeck processes. For this particular process, let $\theta_t^{(1)} = \rho_t$ and $\theta_t^{(2)} = s_t$. In order to obtain a time-varying version of the Ornstein-Uhlenbeck process, equation \eqref{3.sdebasic2} must be modified by setting $\mu(Y_t, \bm{\theta}_t) = \rho_t(\omega_c - Y_t)$ and $\sigma(Y_t, \bm{\theta}_t) = s_t$. The modification leads to 
\begin{align}\label{3.ou1}
    dY_t = \rho_t(\omega_c - Y_t)dt + s_tdW_t,\thickspace\thickspace\thickspace\thickspace Y_0 = y_0.
\end{align}
In equation \eqref{3.ou1}, $\omega_c$ can be viewed as a mean-reverting parameter.\footnote{And in fact, to be consistent with section \ref{CTCRW_SEC}, $\omega_c$, again, is a mean velocity, meaning that the defined process tends to revert to this centre of attraction.} Again, using the Euler-Maruyama discretization for the time-varying Ornstein-Uhlenbeck process leads to an approximate density
\begin{align}\label{3.euler-maruyama2}
    p(Y_{t + \Delta} = y_{t + \Delta} \mid Y_t = y_t) = \phi(y_{t+\Delta};\thickspace \omega_c + e^{-\rho_t\Delta}(y_t - \omega_c), \frac{s_t^2}{2\rho_t}(1-e^{-2\rho_t\Delta}),
\end{align}
which can then again be substituted into equation \eqref{3.sdebasic5} in order to get the approximate likelihood for the time-varying Ornstein-Uhlenbeck process \citep{Michelot2021}. Additionally, one can note the exact analogy to equations \eqref{3.0ctcrw2} and \eqref{3.0ctcrw3}, which define an Ornstein-Uhlenbeck velocity process, meaning that equation \eqref{3.euler-maruyama2} is a time-varying version of a CTCRW. As already mentioned, when fixing $\omega_c$ to zero, the time-varying Ornstein-Uhlenbeck process can be used to model the velocity of an animal. Fixing $\omega_c$ to zero is intended to avoid any systematic bias in the velocity.

Turning back to the general likelihood as given in equation \eqref{3.sdebasic5}, penalising the smoothing terms leads to the penalised log-likelihood 
\begin{align}\label{3.sdebasic6p}
    l_p(\bm{\xi}, \bm{\beta}, \bm{\lambda} \mid  y_1,\mathellipsis, y_n) = \text{log}(L(\bm{\xi}, \bm{\beta} \mid  y_1,\mathellipsis, y_n)) - \sum_j\lambda_j\bm{\beta}_j^T\bm{S}_j\bm{\beta}_j.
\end{align}
As seen in section \ref{GAMz_theory}, the last term $\sum_j\lambda_j\bm{\beta}_j^T\bm{S}_j\bm{\beta}_j$ in equation \eqref{3.sdebasic6p} is the penalisation of the smoothing splines to account for the bias-variance trade-off in the relationship between the parameter vector $\bm{\theta}_t$ and the covariates in the model. In particular, $\lambda_j$ is a penalisation parameter for the $j$-th smooth term in $\bm{\theta}_t$. The matrix $\bm{S}_j$ is a matrix of known covariates in the model, and $\bm{\beta}_j$ is the vector of the basis coefficients as stated in equation \eqref{3.gam2}. Thus, $\bm{\beta}_j^T\bm{S}_j\bm{\beta}_j$ is a measure for the roughness (wiggliness) of the $j$-th smoothing term. When the likelihood given in equation \eqref{3.sdebasic6p} has been maximised\footnote{The likelihood itself is implemented using a Kalman filter with time-varying parameters, which is basically the same procedure as described in section \ref{CTCRW_SEC}.}, the fitted model must be checked and validated in order to select the best model among a number of potential candidates \citep{Michelot2021}. Figure \ref{CTCRW_RHO_SIGMA} provides an example of parameter estimation of a time-varying CTCRW, where $\rho$ and $s$ have been estimated. Both parameters depend on the covariate 'temp'. The red lines indicate 100 posterior draws, following the idea of \citet{nychka1988bayesian}, in order to generate confidence intervals. 
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{BETA_SIGMA_EST.pdf}
\caption[Estimated parameters for time-varying CTCRW]{Estimated parameters for time-varying CTCRW\\
Source: Own representation}
\label{CTCRW_RHO_SIGMA}
\end{center}
\end{figure}

At this point, all basics for the further procedure in this work have been introduced. In the following, the methods introduced above will be used to model the tracking data and gain more insights.
\clearpage
% % % % % % % % % % % % % % % % % % % % % % % %
% % % Results % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %

\chapter{Results}
This chapter provides the results of the methods described in the previous chapter. As in the previous chapter, the results are divided into three logical blocks. The results of the GAMMs are described first, followed by those of the HMMs and finally the outcome of the SDEs. All computations were performed on a 4.9 GHz Intel Core i7 9700k and 16GB DDR4 RAM using the statistical software R (\citet{team2013r}). The focus in all three blocks is to model the relationship between animal movement characteristics, such as speed, step length and turning angles, and environmental covariates like temperature and time of day.

% % % % % % % % % % % % % % % % % % % % % % % %
% % % GAMMs % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %

\section{Generalised Additive Models}\label{GAMMz}
The first method applied to data is a GAMM in order to have a direct comparison to the GAMM fitted by \citet{thaker2019}. The GAMM output by \cite{thaker2019} and its model formulation can be seen in Appendix \ref{AnhangA}. The GAMM in this work models the relationship between speed of movement (in km/h) and collar temperature. Speed of movement corresponds to the variable 'speed', whereas the collar temperature corresponds to the covariate 'temperature' (or 'temp', respectively). Additionally, 'season' was included as a categorical fixed effect as well as the animal ID and hour (time of day) as random effects. The season is divided into two categories: dry season (May to October) and wet season (November to April). Moreover, because consecutive observations of the movement path are correlated, autocorrelation in the response variable is modeled via an AR(1) process. In contrast to the GAMM by \cite{thaker2019}, woody density was not included, because this information is not available. Apart from using an imputed dataset and including autocorrelation, this is the only difference in the model formulation. Thus, the first GAMM in this work can be compared to the GAMM by \citet{thaker2019} in order to determine the impact of an imputed dataset and including autocorrelation.

Since this is a comparison of basically the same model, the choice of the best model candidate was limited to the choice of the spline basis parameter $k$, which equals the number of basis functions for a certain smoothing function $g$. If the number of basis functions is too small, the model will most likely oversmooth and tend to not capture any patterns in the data. Table \ref{tab:choiceofk} shows several possible choices for $k$. According to AIC and the adjusted $R^2$, a choice of $k = 10$ leads to the best fitted model, and thus, is sufficient. However, a choice of $k = 15$ is not performing any worse.\footnote{As an alternative to specifying multiple GAMM candidate models and evaluate an appropriate choice of $k$ by AIC, in Appendix \ref{AnhangC} a more sophisticated method is shown.} The model itself was built using the function $\mathtt{bam}$ from the package $\mathtt{mgcv}$ written by \citet{wood2015package}. The model was fitted using PIRLS, whereas the estimation of the smoothing parameter was done via REML. Additionally, the AIC was calculated using the conditional likelihood of the model (consequently the AIC is the conditional AIC) and the correction to the EDF proposed by \citet{wood2016smoothing} was taken into account.

\begin{table}[htb]
\setlength{\tabcolsep}{36pt}
  \centering
  \caption{Model selection - Choice of k}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}
                   lll
                            }
    \toprule
    Choice of $k$ & AIC & adj. $R^2$\\ 
    \midrule
 $k = 5$ & 1499149 & 0.0677 \\ 
 \boldmath{$k = 10$} & \textbf{1499061} & \textbf{0.0687} \\
 $k = 15$ & 1499064 & 0.0687 \\
 \bottomrule
    \end{tabular*}%
  \label{tab:choiceofk}%
  \begin{tablenotes}[flushleft]\small
  \source   Own representation
  \end{tablenotes}
\end{table}%

Figure \ref{GAM_fitted} represents the fitted GAMM. The mean speed by temperature was calculated for each degree Celsius and classified by season, with triangles representing the wet season and circles the dry season. The blue line represents a fitted temperature spline for the wet season, whereas the red dashed line represents a fitted spline for the dry season. It can be seen, that elephants moved faster in the wet season than in the dry season, which is consistent to the GAMM by \citet{thaker2019}. The output summary of the model can be found in Appendix \ref{AnhangA}.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{GAM_final.pdf}
\caption[Fitted GAMM]{Fitted GAMM\\ Source: Own representation}
\label{GAM_fitted}
\end{center}
\end{figure}

However, Figure \ref{GAM_fitted} indicates a bigger difference between the mean speed by season than indicated by the GAMM fitted by \citet{thaker2019}, which can be seen from the vertical distance between the splines. Also, the confidence intervals of the mean speed by temperature for both seasons, as well as for the smoothing splines by season are higher. In fact, there are three reasons for the differences: The imputed dataset, the autocorrelation that is taken into account, and omitting the covariate 'woody density'. Considering the first reason, Table \ref{tab:diffgam} represents the deviance in the mean speed by season. The calculation for the mean speed by season is independent of the GAMM fitted and therefore, the deviation can be related to the imputed dataset. While the mean movement speed in the wet season is faster, it is slower in the dry season compared to the mean speed computed by \citet{thaker2019}. The standard deviation of the mean speed in this work is about the same for the wet season and significantly lower in the dry season.

\begin{table}[htb]
  \centering
  \caption{Mean Speed by season - Differences}\label{tab:diffgam}
  \setlength\tabcolsep{0pt}
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}
                        l
                   *{3}{S[table-format= 3.3]}
                   *{3}{S[table-format= 3.3]}
                            }
    \toprule
    &   \multicolumn{3}{c}{Mean}   &   \multicolumn{3}{c}{Standard deviation}   \\
    \cmidrule(r){2-4}
    \cmidrule(l){5-7}
    &   {\thead[b]{Wet}}
        &   {\thead[b]{Dry}}
        &   {\thead[b]{Difference}}
                &   {\thead[b]{Wet}}
                    &   {\thead[b]{Dry}}
                    &   {\thead[b]{Difference}}\\
    \midrule
Mean Speed Meyer    & 0.47    & 0.33 & 0.14 & 0.50 & 0.37 & 0.13           \\
Mean Speed \citet{thaker2019}
        & 0.42  & 0.39 & 0.03 & 0.49 & 0.46 & 0.03           \\
Difference & 0.05 & -0.06 & / & 0.01 & -0.09 & / \\
    \bottomrule
  \end{tabular*}
  \begin{tablenotes}[flushleft]\small
  \note   Wet and Dry correspond to the possible categories of the covariate 'season'. Values are in km/h.
  \source   Own representation
  \end{tablenotes}
\end{table}

The reason for the deviance is that \citet{thaker2019} did not account for the irregularity when computing the mean speed. The imputed dataset in this work does account for irregularity and hence shows a different mean speed because the calculated speed is less biased by the distance traveled. Additionally, the imputation also influences the confidence intervals in the mean speed by temperature shown in Figure \ref{GAM_fitted}.

Considering the GAMM splines in Figure \ref{GAM_fitted}, there are also wider confidence intervals than indicated in the model by \citet{thaker2019}. The reason for a wider confidence interval (besides the mentioned speed differences in the datasets) is due to the fact that autocorrelation is taken into account in the model of this work. If the observations are assumed to be uncorrelated when they are actually correlated, the standard errors often are underestimated and the confidence interval would be too narrow (see \citet{Wood2017}, Ch. 7). 

Autocorrelation itself is another aspect to note. Figure \ref{GAM_ACF} shows the autocorrelation function (ACF) of the residuals of the fitted GAMM with a pattern that repeats itself every 24 hours. The pattern indicates that there still is an effect not covered in the GAMM. Since this seems to be a daily pattern, an AR process of higher order might solve this issue. The function $\mathtt{bam}$ can model autocorrelation via an AR(1) process, however, it cannot model the general AR(p) case. 
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{GAM_ACF.pdf}
\caption[ACF-Plot GAMM]{ACF-Plot GAMM\\ Source: Own representation}
\label{GAM_ACF}
\end{center}
\end{figure}
There are functions in R that are theoretically capable of modeling the AR(p) case\footnote{For example, the function $\mathtt{brm}$ from the package $\mathtt{brms}$ and the function $\mathtt{gamm}$ from the package $\mathtt{mgcv}$ are valuable alternatives.}, but already AR(2) and AR(3) processes are computationally very demanding and most computers do not have enough RAM to compute the autocorrelation matrices needed. Nonetheless, autocorrelation is a crucial aspect of movement data, that \citet{thaker2019} ignored when modeling the relationship between movement speed and temperature. Concerning the 'woody density', the effect of omitting this covariate cannot be measured, since the data are not publicly available. 

Up to this point, the two GAMMs were compared to determine the effect of the imputed dataset and the impact of accounting for autocorrelation in the data. However, as can be seen in the model formulation, \cite{thaker2019} included the covariate 'hour' as a random effect in the GAMM in order to model an hourly effect. Since there can still be seen a 24-hour cycle in the ACF-Plot represented in Figure \ref{GAM_ACF}, it makes sense to reconsider the implementation of such a daily pattern. In particular, \citet{thaker2019} used the covariate 'hour' as a random effect. However, a cyclic cubic regression spline (or a cyclic penalised regression spline, respectively) to model the effect of a daily pattern might be advantageous in model fitting \citep{Wood2017}, and will be used in the following. Another issue is the assumed distribution of the response variable. \citet{thaker2019} assumed that the response variable 'speed' is normally distributed. However, since the variable 'speed' does not take on negative values at all, a Gamma distribution is more appropriate. Therefore, in the following, two GAMMs will be fitted to further investigate the interaction between individuals speed and their environmental system. In the first GAMM, a Gaussian distribution is assumed, whereas in the second GAMM a Gamma distributed response variable is assumed. Moreover, $h$ is a log link function.

For model selection, Table \ref{tab:aicgam} shows multiple model variations of GAMMs, that assume a normal distributed response variable or a Gamma distributed response variable, respectively. In fact, the full models both consist of covariates 'temperature' and 'hour', modelled as splines, while 'hour' is a cyclic cubic regression spline. Covariate 'season' is a categorical fixed effect, and 'ID' is a random effect. Moreover, the autocorrelation structure is, as before, included in the models. The GAMM seen in Figure \ref{GAM_fitted} results in a conditional AIC of $1499061$, whereas the full models seen in Table \ref{tab:aicgam} yield a conditional AIC of $1494092$ and $1374824$, respectively, whereas both are lower. That means, instead of formulating time of day as a random effect, it is advantageous to use a cyclic regression spline to represent a daily pattern in the speed distribution. Moreover, assuming a Gamma distributed response variable further reduces the AIC. Omitting the covariates 'season', 'temperature', 'id' and 'hour' results in a lower AIC. The AR(1) process is also essential, as without it a higher AIC results.

\begin{table}[htb]
  \centering
  \caption{GAMM (gaussian) - Model Selection by AIC}\label{tab:aicgam}
  \setlength\tabcolsep{0pt}
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}
                        l
                   *{5}{S[table-format= 2.3]}
                   *{2}{S[table-format= 5.3]}
                            }
    \toprule
    &   \multicolumn{5}{c}{Covariates/Structure omitted}   &   \multicolumn{2}{c}{AIC}   \\
    \cmidrule(r){2-6}
    \cmidrule(l){7-8}
&   {\thead[b]{temperature}}
        &   {\thead[b]{season}}
        &   {\thead[b]{ID}}
                &   {\thead[b]{time of day}}
                    &   {\thead[b]{AR(1)}}
                    &   {\thead[b]{Gaussian}}
                    &   {\thead[b]{Gamma}}\\
    \midrule
\textbf{Candidate 1} &  &  &  &  &  & \textbf{1494092} & \textbf{1374824}          \\
\text{Candidate 2} & \text{omitted}  &  &  &  &  &  \text{1494435} &  \text{1375054}        \\
\text{Candidate 3} &  & \text{omitted} &  &  &  &  \text{1494792} &  \text{1375666}        \\
\text{Candidate 4} &  &  & \text{omitted}  &  &  &  \text{1495114} &  \text{1376071}         \\
\text{Candidate 5} &  &  &  & \text{omitted}&  &  \text{1499142} &  \text{1386532}         \\
\text{Candidate 6} &  &  &  &  & \text{omitted}  &  \text{1500972} &  \text{1381317}         \\
    \bottomrule
  \end{tabular*}
  \begin{tablenotes}[flushleft]\small
  \note   For the calculation of the conditional AIC, the correction to the EDF proposed by \citet{wood2016smoothing} was taken into account
  \source   Own representation
  \end{tablenotes}
\end{table}

The resulting GAMMs are represented in Figure \ref{GAMMAS_COMPARED_TEMP}. The output summaries can be found in Appendix \ref{AnhangA}. Comparing both GAMMs to the GAMM by \cite{thaker2019}, the adjusted $R^2$ is significantly higher: 0.119 for the Gaussian, 0.125 for the Gamma GAMM vs 0.067 for the GAMM fitted by \citet{thaker2019}. According to the GAMM splines, the fit looks better than the previous GAMMs. Furthermore, the Gamma distributed GAMM has slighty improved smoothing splines. The effect is mainly the same; when it gets hotter, the elephants tend to increase their movement speed up to 40° Celsius. At 40° Celsius and hotter, however, their movement speed slightly decreases.

\begin{figure}[H]
\begin{center}
\includegraphics[width=1\textwidth]{GAMMAS_COMPARED_TEMP.pdf}
\caption[GAMMs using cyclic cubic regression splines]{GAMMs using cyclic cubic regression splines\\ Source: Own representation}
\label{GAMMAS_COMPARED_TEMP}
\end{center}
\end{figure}

Considering the ACF-Plots represented in Figure \ref{ACF_PLOTS_GAMMS_COMPARED}, there is even less autocorrelation left in the residuals, indicating, that the cyclic implementation of the covariate 'hour' better explains the cyclical pattern than 'hour' implemented as a random effect. However, there is still some autocorrelation left in the residuals.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{ACF_PLOTS_GAMMS_COMPARED.pdf}
\caption[GAMM ACF-Plots using cyclic cubic regression splines]{GAMM ACF-Plots using cyclic cubic regression splines\\ Source: Own representation}
\label{ACF_PLOTS_GAMMS_COMPARED}
\end{center}
\end{figure}

Another aspect that might could help to further reduce the autocorrelation pattern still present in the ACFs would be to include the covariates 'time of day' (or 'hour', respectively) and 'temperature' in the CTCRW that is used to impute the dataset. Nevertheless, the functions $\mathtt{crwFit}$ and $\mathtt{crwPredict}$ will not impute any missing covariate values. Since the covariate values are missing, they must be imputed outside of the package $\mathtt{crawl}$. However, this procedure is beyond the scope of this work and it is questionable whether it makes sense to impute missing covariate data over several days, weeks or even months, as this would correspond to guesswork at best (besides that, as already mentioned, there is also an errors-in-variables problem).

Figure \ref{QQ_PLOT_COMPARISON} shows Quantile-Quantile-Plots (QQ-Plots) for the fitted GAMMs. The function for generating the QQ-Plot is called $\mathtt{qq\_plot}$ from the package $\mathtt{gratia}$ written by \citet{simpson2018r}. For the QQ-Plot illustrated in Figure \ref{QQ_PLOT_COMPARISON}, the deviance residuals are standardised and sorted. If the residuals were standard normally distributed, they would form a straight line, interfered by some random noise \citep{Wood2017}. This reference case is indicated by the red line. As can be seen from the deviance of the residuals, the overall fit of the GAMM is still not satisfying and improvable. However, there is a significant improvement when assuming a Gamma distributed response variable.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{QQ_PLOT_COMPARISON.pdf}
\caption[GAMM normal QQ-Plots]{GAMM normal QQ-Plots\\ Source: Own representation}
\label{QQ_PLOT_COMPARISON}
\end{center}
\end{figure}
Figure \ref{RES_COMPARISON_GAMM} represents another view of the deviance residuals. As can be seen, the residuals for the Gamma distributed GAMM are much less skewed than the residuals for the normal distributed GAMM and in fact do not indicate a severely bad fit.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{RES_COMPARISON_GAMM.pdf}
\caption[GAMM - Residuals]{GAMM - Residuals\\ Source: Own representation}
\label{RES_COMPARISON_GAMM}
\end{center}
\end{figure}

Put together, there are three aspects to note for the GAMM. The first aspect refers to the imputation of the dataset itself. Since there is a notable difference between the results of the GAMM fitted by \citet{thaker2019} and the first GAMM in this work, it remains unclear which of the two datasets leads to better results in the sense of the real data-generating process modeled via GAMMs. The imputed datset improves some irregularities, however, when the temporal distance between two consecutive observations becomes too large, it fails to model an appropriate path for this particular time interval and will bias the calculated speed. To overcome this issue, SDEs can be used as an alternative approach, to avoid the need to impute missing data.

The second aspect concerns the autocorrelation. ACF-Plots might help to chose an appropriate correlation structure to be used in a GAMM, however, HMMs offer a more natural alternative to deal with autocorrelation due to their properties. Regarding the implementation of cyclic functions to model a daily pattern, this procedure seems to improve the model quality and could also be used in HMMs. 

The last aspect relates to the overall model formulation. GAM(M)s are a very flexible and powerful tool, as they are relatively simple to interpret and the regularisation helps to avoid overfitting \citep{larsen2015gam}. Moreover, there is no need to specify any parametric distribution for the covariate effects and in fact, movement processes are most often nonlinear \citep{patterson2017statistical}. However, this simultaneously implies that more caution is required when setting up a model. This includes choosing an appropriate distribution of the response variable, thoughtfully selecting splines (basis dimension, the correct choice of splines etc.) and checking for autocorrelation.

% % % % % % % % % % % % % % % % % % % % % % % %
% % % HMMs % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %

\section{Hidden Markov Models}\label{HMM_SEC}
HMMs offer a more natural way to deal with autocorrelation since they constitute a flexible tool to handle time series data. A key aspect concerning the GAMM formulation, is that ecological systems are subject of certain changes among latent system states through time. The animals of these ecological systems can be driven by these state dynamics and hence affect their behaviour. HMMs are able to accommodate complex structures that account for certain changes between latent system states and thus offer an appropriate choice for modeling animal tracking data \citep{mcclintock2020uncovering,ephraim2002hidden,Zucchini2016}. Thus, the focus of this section is to gain further knowledge of the animals interaction with their environment. In contrast to section \ref{GAMMz}, the animals step length and their turning angles are modeled by taking advantage of the properties of HMMs, which is to assume latent environmental states to drive the animals behaviour. As already implied, the underlying assumptions for an HMM are regular spaced observations in time, as well as an negligible error \citep{patterson2017statistical}. Both assumptions hold approximately true. The imputed dataset contains only equidistant observation times and those are based on GPS technology, which is sufficiently accurate. 

The fitted HMMs in this work are extensions of the basic HMM framework. Since the dataset contains step lengths (which can be considered a substitute for speed due to equidistant time intervals) and turning angles, the HMMs can be used to model multivariate time series that will be driven by the same underlying state process. The first component series, step lengths, are assumed to follow a Gamma distribution with mean $\mu$ and variance $\sigma^2$, because this has proven to be advantageous in the GAMM modeling and, moreover, step length also cannot take on negative values. The second series component, turning angles, are assumed to follow a von Mises distribution with mean $\mu$ and concentration parameter $\tau$. The model formulation, thus is given by
\begin{align}\label{4.2hmm1}
Y_{t1} \mid C_t = & j \sim \Gamma(\mu_j^{\text{step}}, \sigma_j^{\text{step}}), \nonumber \\ & Y_{t2} \mid C_t = j \sim \text{von Mises}(\mu_j^{\text{turn}}, \tau_j^{\text{turn}}),\thickspace\thickspace\thickspace\thickspace Y_{t1} \perp Y_{t2} \mid C_j.
\end{align}
Equation \eqref{4.2hmm1} describes a model that assumes contemporaneous conditional independence, meaning that the state-dependent distribution $p_j(y)$ is just the product of the corresponding marginal probabilities. However, the Markov chain still induces serial dependence and cross-dependence in the component series $Y_{t1}, Y_{t2}$ meaning that contemporaneous conditional independence does neither imply serial nor mutual independence in the component series \citep{Zucchini2016}. Nonetheless, contemporaneous conditional independence makes it easier to find suitable distributions because one can select classes of univariate distributions instead of having to find multivariate distributions.

Additionally, the HMMs are modeled containing covariates in the state process, meaning that the transition probabilities are expressed as a function of covariates. The reason for this model formulation is, that it is usually of interest to relate the state-switching process to environmental covariates, in order to draw conclusions of how animals interact with their environment \citep{patterson2017statistical}. For all HMMs fitted in this work, the transition probabilities are affected by the covariates 'temperature', 'hour' and their interaction term to examine how the animals respond to these environmental factors (i.e. how state switching depends on these external factors). As already mentioned in section \ref{GAMMz}, there is a within-day variation, as can be seen in Figure \ref{GAM_ACF}. This within-day variation can also be well modelled using trigonometric functions instead of a cyclic regression spline. To achieve this, a certain type of a so called cosinor function will be applied \citep{cornelissen2014cosinor}.

When it comes to model fitting, the first question that arises with HMMs is how many states should be chosen. In this work, three HMMs were fitted with two, three and four states. The transition probabilities of all three HMMs depend on the covariates 'temperature', $cosinor(\text{'hour'})$ and their interaction term. All HMMs in this work have been computed using the package $\mathtt{momentuHMM}$, written by McClintock and Michelot \citep{mcclintock2018momentuhmm}. Table \ref{tab:choiceofstates} shows the AIC and BIC of the corresponding number of states chosen. As can be seen, both AIC and BIC tend to select the three-state HMM. This is also in common with most biologists opinions who suggest that there should be three states, which could be 'Resting', 'Foraging' and 'Traveling' \citep{langrock2014modelling, patterson2008state}. Nonetheless, the states should be considered with caution, as they are rather proxies for animals true state processes \citep{patterson2017statistical}.

\begin{table}[htb]
\setlength{\tabcolsep}{36pt}
  \centering
  \caption{Model selection - Number of States}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}
                   lll
                            }
    \toprule
    Number of States & AIC & BIC \\ 
    \midrule
 $m = 2$ & 1659949 & 1660073 \\ 
 \boldmath{$m = 3$} & \textbf{1640022} & \textbf{1640498} \\
 $m = 4$ & 1640966 & 1641375 \\
 \bottomrule
    \end{tabular*}%
  \label{tab:choiceofstates}%
  \begin{tablenotes}[flushleft]\small
  \source   Own representation
  \end{tablenotes}
\end{table}%

The next aspect is whether the transition probabilities should be affected by 'temperature' and $cosinor(\text{'hour'})$ or just one of the two covariates and if there should be any interaction term. In Table \ref{tab:choiceoftransition}, four three-state HMMs with different functions for the transition probabilities are shown. 
\begin{table}[htb]
\setlength{\tabcolsep}{32pt}
  \centering
  \caption{Model selection - Covariates in transition probabilities}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}
                   lll
                            }
    \toprule
    Covariates in transition probabilities & AIC & BIC \\ 
    \midrule
 $cosinor(\text{'hour'})$ & 1641452 & 1641757 \\
 'temp' & 1646528 & 1646775 \\
 'temp' + $cosinor(\text{'hour'})$ & 1643667 & 1644029 \\
 \textbf{'temp' + $\mathbf{cosinor(\text{'hour'})}$ + interaction} & \textbf{1640022} & \textbf{1640498} \\
 \bottomrule
    \end{tabular*}%
  \label{tab:choiceoftransition}%
  \begin{tablenotes}[flushleft]\small
  \source   Own representation
  \end{tablenotes}
\end{table}%
According to AIC and BIC, the model with both covariates including an interaction term fits the data best. It is worth noting that the model with both covariates but without any interaction term fits worse than the model including cosinor('hour') only, as it seems to be overly complex.

As a model checking step, it must be evaluated how well the model explains the data. Figure \ref{HMM_densities} represents the state-dependent Gamma densities for the step lengths in metres for all animals. The orange line corresponds to the 'Resting' state and indicates that, given the animals are in the resting state, the expected step length per hour is about 70 metres. Likewise, the blue and green lines represent the 'Foraging' and the 'Traveling' state. The expected step length per hour in the foraging state is 236 metres and 694 metres in the traveling state respectively. The black dotted line shows the overall density. As the grey bars represent the observed step lengths of the animals, it can be seen that the total density fits the data quite well. The model output for the HMM can be seen in Appendix \ref{AnhangB}.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{HMM_densities.pdf}
\caption[HMM - State-dependent densities for step lengths]{HMM - State-dependent densities for step length\\ Source: Own representation}
\label{HMM_densities}
\end{center}
\end{figure}

In the same manner, Figure \ref{HMM_densities_turning} represents the state-dependent densities for the turning angles. In the 'Foraging' and 'Traveling' state, the densities are centered around an angle of zero, whereas in the 'Resting' state, the density rather tends to be uniformly distributed. Considering the size of the animals, it intuitively makes sense that the animals tend to not turn that much when traveling bigger distances but rather tend to turn more in the 'Resting' state.
\begin{figure}[H]
\begin{center}
\includegraphics[width=1\textwidth]{HMM_turning_angles.pdf}
\caption[HMM - State-dependent densities for turning angles]{HMM - State-dependent densities for turning angles\\ Source: Own representation}
\label{HMM_densities_turning}
\end{center}
\end{figure}

Regarding both covariates, they most likely are positively correlated. However, if their correlation was a serious issue, the transition probabilities would show wide confidence intervals. Figure \ref{HMM_state_transitions} shows the stationary state probabilities. Indeed, the state probabilities depend on both covariates, however, the covariate 'temperature' is fixed to its mean value (27.48° Celsius). Hence, Figure \ref{HMM_state_transitions} represents the state probabilities per hour, when the covariate 'temperature' is held fixed. 
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{HMM_Stationary_state_probabilities_temp.pdf}
\caption[HMM - Stationary State probabilities by hour]{HMM - Stationary State probabilities by hour\\ Source: Own representation}
\label{HMM_state_transitions}
\end{center}
\end{figure}
The confidence intervals are not overly wide and thus a positive correlation of both covariates does not seem to be an issue here. Additionally, a certain pattern emerges. In the morning, the animals travel farther distances, forage in the evening and tend to rest at night.

Likewise, the state probabilities can be shown by temperature, as can be seen in Figure \ref{HMM_TEMP_STATE_PROBS}. Again, the dimension for the covariate 'hour' is held fixed. The Plot shows that the animals most likely occur to be in the 'Foraging' or 'Traveling' state at noon at thus, travel rather farther distances. In Appendix \ref{AnhangB}, a reference to an animated version of the stationary state probabilities can be found.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{HMM_TEMP_STATE_PROBS.pdf}
\caption[HMM - Stationary State probabilities by temperature]{HMM - Stationary State probabilities by temperature\\ Source: Own representation}
\label{HMM_TEMP_STATE_PROBS}
\end{center}
\end{figure}

For a further step in model checking, Figure \ref{HMM_qqacf} graphically shows the corresponding QQ-Plots as well as the ACF-Plots for the step lengths and the turning angle, respectively. The first row represents the output for the step lengths and the second row the output for the turning angles. As can be seen from the QQ-Plot, the pseudo-residuals tend to be standard normally distributed. There seems to be a minor lack of fit in the outer areas, but overall it appears to be a good fit. Moreover, comparing the QQ-Plot with the QQ-Plot of the fitted GAMM in Figure \ref{QQ_PLOT_COMPARISON}, a major improvement can be seen. The ACF-Plot for the turning angles does not show any autocorrelation, however, there still is some pattern left in the ACF for the step lengths. Comparing the ACF for the best fitted GAMMs seen in Figure \ref{ACF_PLOTS_GAMMS_COMPARED} and the step lengths ACF for the HMM, the HMM can also handle the autocorrelation in a decent way as there is not much autocorrelation left in the residuals.

All in all, the HMM offers a broad variety of output and could even be tuned further. For example, random effects for the animals could be implemented and more covariates (if accessable) could be added to the transition probabilities function. The cyclical pattern has been explained even better by the HMM than the GAMM. Although there is still some room for improvement, the general fit of the HMM is better than the fit of the GAMM. 
\begin{figure}[H]
\begin{center}
\includegraphics[width=1\textwidth]{HMM_2cov_with_interaction_term.pdf}
\caption[QQ and ACF-Plots of step length and turning angle]{QQ and ACF-Plots of step length and turning angle\\ Source: Own representation}
\label{HMM_qqacf}
\end{center}
\end{figure}
\noindent Nonetheless, both methods, HMMs and GAMMs have in common, that they heavily rely on equidistant observations. For HMMs, the discrete-time Markov chains are not interpretable without any constant sampling unit, because, based on irregular sampling, there is no way to formulate an appropriate distribution for the response variable that takes time passed into account \citep{patterson2017statistical}. As mentioned in section \ref{GAMMz}, there still is some chance of the CTCRW not to catch every trend in the original dataset and thus might bias the imputed dataset. 
% % % % % % % % % % % % % % % % % % % % % % % %
% % % SDEs % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %
\section{Stochastic Differential Equations}
In order to model the relationship between movement speed and environmental covariates, an approach that does not require the imputation of data, is the time-varying SDE method. In comparison to simply using GAMMs for modeling tracking data, the time-varying SDE approach is more mechanistic, in the sense that it describes the mechanisms behind animal movement behaviour. As already mentioned, the parameters $\mu$ and $\sigma$ determine the type of SDE. The CTCRW is not only suitable for imputing movement paths as has been seen in section \ref{CTCRW_SEC}, but also is an option in this context, because it takes a certain inertia or autocorrelation between the increments into account. Thus, a CTCRW is used in the first place. 

For the models shown in this chapter, $\mu$ and $\sigma$ only depend on one time-varying parameter each, i.e. $\mu(Y_t, \bm{\theta}_t) = \mu(Y_t, \theta_t^{(1)})$ and $\sigma(Y_t, \bm{\theta}_t) = \sigma(Y_t, \theta_t^{(2)})$, where $\bm{\theta}_t = (\theta_t^{(1)}, \theta_t^{(2)})$ (see equation \eqref{3.sdebasic2} for general reference of a time-varying SDE formulation). To be consistent with the CTCRW in section \ref{CTCRW_SEC}, let $\rho_t = \theta_t^{(1)}$ and $s_t = \theta_t^{(2)}$. Since the animals move through space, there are two dimensions (longitude and latitude) and hence, the time-varying velocity process is bivariate. However, there is no need to specify a two dimensional parameter vector, because the velocity process itself is isotropic, and consequently can be considered as driven by the same underlying parameters \citep{Michelot2021}. In the CTCRW case, the speed, denoted as $\nu$, can be calculated via $\nu_t = \sqrt{\pi}s_t/(2\sqrt{\rho_t})$ \citep{Michelot2021, gurarie2017correlated}.\footnote{For a detailed derivation of this claim, see \citet{gurarie2017correlated}.}

As previous results show, the animals movement paths are affected by temperature and time of day. Thus, in the first place, the parameters $\rho$ and $s$ both depend on the covariate 'temperature'. The relationship is modeled using thin-plate regression splines. All time-varying SDEs have been fitted using the $\mathtt{smoothSDE}$ package, written by \cite{michelot2021package}. However, it was not possible to fit a joint model for all individuals, meaning that the model failed to fit shared parameters $\rho$ and $s$ for the 14 individuals. Hence, adding the covariate 'hour' into the model to account for a cyclical pattern does not make any sense either. The failure could have several reasons. One reason might be, that the elephants have too different movement patterns. Another, related reason, could be a too divergent reaction to temperature.

When no model fits all individuals at once, it makes sense to fit a model for every 14 animals separately. Nevertheless, it is not possible to fit a model with the entire movement path of each elephant. In particular, there are only piecewise sequences that could be fitted using the covariate 'temperature'. As an example, this is shown in the following for the individual 'AM254'. The sequence is limited to May 2008 up to October 2008 and thus corresponds to the dry season. 

Figure \ref{SDE Parameters for AM254} shows the fitted parameters $\rho$ and $s$. The red lines indicate 100 posterior samples. Following the idea of \citet{nychka1988bayesian}, these posterior draws create a confidence interval for the fitted spline (black line). As can be seen, $\rho$ increases when temperature increases. The same holds true for $s$, whereas a steep decrease can be observed, when the temperature is above 35° Celsius. The decrease might be due to the fact, that this could be too hot for the individual to move fast. 
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{AM254_PARAMETERS_SDE.pdf}
\caption[SDE Parameters for AM254 by temperature]{SDE Parameters for AM254 by temperature\\ Source: Own representation}
\label{SDE Parameters for AM254}
\end{center}
\end{figure}

Moreover, Figure \ref{Fitted_SDE_AM254} represents the relationship between speed and temperature. The speed, $\nu_t$, can be seen at the ordinate, whereas the temperature is shown at the abscissa. Comparing the result of this SDE to the GAMM in Figure \ref{GAMMAS_COMPARED_TEMP}, the result is very similar. The hotter it gets, the faster the elephant is moving. However, as it gets too hot, the elephant decreases its movement speed, whereas this relationship could already been observed in Figure \ref{SDE Parameters for AM254}. 
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{AM254_TEMP_SDE.pdf}
\caption[Fitted SDE AM254 by temperature]{Fitted SDE AM254 by temperature\\ Source: Own representation}
\label{Fitted_SDE_AM254}
\end{center}
\end{figure}

Nonetheless, the overall result is not satisfying, because the model cannot explain the entire movement path of the animal using the covariate 'temperature'. Thus, as a next step, the covariate 'hour' is used to capture the daily pattern of the animals. The parameters $\rho$ and $s$ are modeled using the covariate 'hour' via a cyclic P-spline. Again, the model fails to explain all 14 elephants with two shared parameters. However, using this model formulation, it was possible to fit a model for each animal without limiting the movement path to a certain sequence. As an example, Figure \ref{AM239_AM253_text} shows the relation between speed and time of day for the animals 'AM239' and 'AM253'. The daily movement pattern is very similar to the movement pattern shown in the HMM, as can be seen in Figure \ref{HMM_state_transitions}. The animals tend to be faster (and hence travel bigger distances) in the early morning and at noon. All 14 fitted models for the time-varying SDEs by time of day can be seen in Appendix \ref{AnhangD}.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{AM239_AM253.pdf}
\caption[AM239 and AM253 SDE Output by time of day]{AM239 and AM253 SDE Output by time of day\\ Source: Own representation}
\label{AM239_AM253_text}
\end{center}
\end{figure}

Turning to model selection for the time-varying CTCRW, theoretically there were three candidates: a time-varying SDE model with both parameters $\rho$ and $s$ depending on time of day and temperature, depending on temperature only, or on time of day only, respectively. Effectively, there is only one candidate remaining, which is the parameters $\rho$ and $s$ to depend on time of day only, because it is the only model that could be fitted (which actually is not even one single model either). Nonetheless, because the covariate is modeled via a cyclic P-spline, a basis parameter $k$ has to be chosen for each model. This can be achieved by formulating multiple time-varying SDEs with a varying number of basis dimensions for the P-spline. Consistent to the GAMM model selection, the conditional AIC, following the correction by \citet{wood2016smoothing} in the EDF, will be used.

Table \ref{tab:choiceofsdedimension} shows as an example different parameter values of $k$ for individual 'AM239'. As can be seen, according to the conditional AIC, $k = 10$ leads to the best fit. For $k = 15$, the log-likelihood is $-\infty$ (the likelihood would be $0$) and thus, no parameter estimation took place. 
\begin{table}[htb]
\setlength{\tabcolsep}{32pt}
  \centering
  \caption{SDE Model selection - Basis functions}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}
                   lll
                            }
    \toprule
    Parameter value $k$ & AIC \\ 
    \midrule
 $k = 5$ & -108340.0 \\
 $\bm{k = 10}$ & -109908.9 \\
 $k = 15$ & \textit{NA} \\
 \bottomrule
    \end{tabular*}%
  \label{tab:choiceofsdedimension}%
  \begin{tablenotes}[flushleft]\small
  \source   Own representation
  \end{tablenotes}
\end{table}%

Considering the results up to this point, due to correlated increments, a CTCRW offers the best characteristics for animal movement. However, the fitting is very problematic. An alternative would be to specify a time-varying Brownian motion in order to fit a model that combines all 14 individuals, although the increments of Brownian motion processes are independent. Again, to be consistent with the Brownian motion formulation in section \ref{SDE_SEC}, let $r_t = \theta_t^{(1)}$ and $s_t = \theta_t^{(2)}$. The parameter $r_t$ can often be fixed to zero, and $s$, the diffusion parameter, will be estimated as a smooth function of temperature or time of day, respectively. In this model, the distribution of $Y_{t+\Delta}$ is normal around $Y_t$, and $s^2$ is proportional to the variance of the distribution, as can been seen in equation \eqref{3.euler-maruyama1}. Consequently, even though the interpretation is not as favourable as the CTCRW, $s$ can be interpreted as a measure of speed, and so the effects of temperature and time of day can be interpreted. 

In fact, using Brownian motion a model including all 14 animals could be fitted. Table \ref{tab:aicbm} basically shows the model selection for two approaches: one approach to model the relationship of speed using the covariate 'temperature' (again, via a thin-plate regression spline), the other approach using time of day (covariate 'hour', modeled using a cyclic regression spline). Using both covariates in one model did not work. According to the conditional AIC, using $k = 10$ basis functions and the covariate 'hour' turned out to be the best model fit. 

\begin{table}[htb]
  \centering
  \caption{Brownian Motion - Model Selection by AIC}\label{tab:aicbm}
  \setlength\tabcolsep{0pt}
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}
                    *{1}{S[table-format= 5.3]}
                   *{2}{S[table-format= 2.3]}}
    \toprule
  \multicolumn{1}{c}{Number of basis functions} &  \multicolumn{2}{c}{Conditional AIC}   \\
    \cmidrule(r){2-3}
    \cmidrule(r){1-1}
        {\thead[b]{Parameter $k$}}
        &   {\thead[b]{temperature}}
        &   {\thead[b]{hour}}\\
    \midrule
\text{$k = 5$} & \text{-9231.2} & \text{-51536.5}          \\
\textbf{$\bm{k = 10}$} & \textbf{-9307.1}  & \textbf{-59936.0}         \\
\text{$k = 15$} &  \textit{NA} & \textit{NA}       \\
    \bottomrule
  \end{tabular*}
  \begin{tablenotes}[flushleft]\small
  \note   For the calculation of the conditional AIC, the correction to the EDF proposed by \citet{wood2016smoothing} was taken into account
  \source   Own representation
  \end{tablenotes}
\end{table}

Figure \ref{Fitted_SDE_BM_PAR} graphically represents the fitted time-varying Brownian motion model, where the parameters depend on time of day. As in Figure \ref{SDE Parameters for AM254}, the red lines indicate a confidence interval via 100 posterior draws. As already mentioned, $r$ does not have a significant influence, as it tends to be around zero, whereas $s$ can be interpreted as a measure for speed. The result is very similar to the CTCRW single fits, indicating that the elephants tend to move faster at morning and at noon. However, Brownian motion is not going to capture as many realistic features of animal movement as the CTCRW: There is a trade-off between realism and flexibility on the one side, and computational stability and speed on the other side.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=1\textwidth]{SDE_PLOTS_HOUR_PAR.pdf}
\caption[Fitted time-varying Brownian motion by temperature]{Fitted time-varying Brownian motion by temperature\\ Source: Own representation}
\label{Fitted_SDE_BM_PAR}
\end{center}
\end{figure}

For model validation, it still is an open question on finding good, general methods for model checking in this context. A simple diagnostic to investigate the goodness-of-fit in time-varying SDE models, is based on the Euler-Maruyama discretization in order to calculate the residuals:
\begin{align}\label{4.3sdebm}
\varepsilon_i =  \frac{y_{i+1}-(y_i+\mu(y_i, \hat{\theta}_{t_i})\Delta_i)}{\sigma(y_i, \hat{\theta}_{t_i})\sqrt{\Delta_i}},
\end{align}
with $i = 1, \mathellipsis n-1$ and $\hat{\theta}_{t_i}$ as an estimate of $\theta_{t}$ over $[t_i, t_{i+1})$. According to the model assumptions, the residuals should be independent and approximately follow a standard normal distribution \citep{Michelot2021}. Figure \ref{Fitted_SDE_BM_QQ} shows a QQ-plot of the residuals against the standard normal distribution. The residuals were calculated using the formula represented in equation \eqref{4.3sdebm}. As can be seen, the residuals are severely skewed and hence, there is evidence that the fit is not sufficient.  
\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{SDE_PLOTS_HOUR_QQ.pdf}
\caption[QQ-Plot for fitted time-varying Brownian motion model]{QQ-Plot for fitted time-varying Brownian motion model\\ Source: Own representation}
\label{Fitted_SDE_BM_QQ}
\end{center}
\end{figure}
This can be confirmed when looking at the ACF of the residuals, which is represented in Figure \ref{Fitted_SDE_BM_ACF}. The Figure shows, that there is positive autocorrelation in the residuals for several hours, turning into a cyclical pattern. The autocorrelation indicates, that some degree of inertia was not captured by the model. As already mentioned, per definition, the increments of a Brownian motion are independent. Consequently it would be adequate to use a CTCRW to account for correlation between the increments. However, a CTCRW could not be fitted containing all individuals in one model.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{SDE_PLOTS_HOUR_ACF_PLOT.pdf}
\caption[ACF for fitted time-varying Brownian motion model]{ACF for fitted time-varying Brownian motion model\\ Source: Own representation}
\label{Fitted_SDE_BM_ACF}
\end{center}
\end{figure}

To sum up, time-varying SDEs offer an intuitive way to analyse tracking data. In particular, a time-varying CTCRW has ideal characteristics to model animal movement. Moreover, time-varying SDEs do not depend on regular observation times, making them a natural choice for irregular tracking data while not being too simplistic, as ordinary SDEs are said to be \citep{Michelot2021, michelot2019state}. The option to implement splines and random effects in the parameters offers a very powerful way to investigate the influence of certain covariates on the process. 

\clearpage
% % % % % % % % % % % % % % % % % % % % % % % %
% % % SUMMARY & DISCUSSION % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %

\chapter{Discussion}
so


% % % % % % % % % % % % % % % % % % % % % % % %
% % % % ENDE % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %
\clearpage

\chapter{Summary and Outlook}
so
\clearpage

\bibliography{library.bib}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\chapter{R-Output - Generalised Additive Models}\label{AnhangA}
\makeatletter
\patchcmd{\verbatim@input}{\@verbatim}{\normalsize\@verbatim}{}{}
GAMM (Family = gaussian) Output by \cite{thaker2019}:
\verbatiminput{Thaker_Output_GAMM.txt}
\clearpage
\noindent GAMM (Family = gaussian) Output (Meyer), hour modeled as random effect:
\verbatiminput{Meyer_Output_GAMM_1.txt}
\clearpage
\noindent GAMM (Family = gaussian) Output (Meyer), hour modeled as a cyclic effect:
\verbatiminput{Meyer_Output_GAMM_2.txt}
\clearpage
\noindent GAMM (Family = gamma) Output (Meyer), hour modeled as a cyclic effect:
\verbatiminput{Meyer_OUTPUT_GAMM_GAMMA.txt}

\clearpage

\chapter{some appendix for the gamm check}\label{AnhangC}

\clearpage

\chapter{R-Output - Hidden Markov Models and animated state probabilities}\label{AnhangB}
\makeatletter
\patchcmd{\verbatim@input}{\@verbatim}{\scriptsize\@verbatim}{}{}
\makeatother

\subsection*{HMM Output}
\verbatiminput{Meyer_Output_HMM.txt} 
$\text{ }$ \\
\subsection*{Animated versions of the stationary state probabilities}
$\text{ }$ \\
Stationary state probabilities by hour: \\
\url{https://timom2110.github.io/Elephants-Moving-Data/hour_gif.html} \\
\\
Stationary state probabilities by temperature: \\
\url{https://timom2110.github.io/Elephants-Moving-Data/temperature_gif.html}


\clearpage

\chapter{R-Output Stochastic Differential Equations}\label{AnhangD}
All time-varying SDE models. Parameters $\rho$ and $\sigma$ depending on covariate 'hour', fitted as a cyclic spline.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{AM105_AM107.pdf}
\caption*{AM105 and AM107 SDE Output by 'hour'\\ Source: Own representation}
\label{AM105_AM107}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{AM108_AM110.pdf}
\caption*{AM108 and AM110 SDE Output by 'hour'\\ Source: Own representation}
\label{AM108_AM110}
\end{center}
\end{figure}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{AM239_AM253.pdf}
\caption*{AM239 and AM253 SDE Output by 'hour'\\ Source: Own representation}
\label{AM239_AM253}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{AM254_AM255.pdf}
\caption*{AM254 and AM255 SDE Output by 'hour'\\ Source: Own representation}
\label{AM254_AM255}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{AM306_AM307.pdf}
\caption*{AM306 and AM307 SDE Output by 'hour'\\ Source: Own representation}
\label{AM306_AM307}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{AM308_AM91.pdf}
\caption*{AM308 and AM91 SDE Output by 'hour'\\ Source: Own representation}
\label{AM308_AM91}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=14cm]{AM93_AM99.pdf}
\caption*{AM93 and AM99 SDE Output by 'hour'\\ Source: Own representation}
\label{AM93_AM99}
\end{center}
\end{figure}
% % % % % % % % % % % % % % % % % % % % % % % % 

\clearpage



% % % % % % % % % % % % % % % % % % % % % % % % 

\chapter*{Versicherung} 
%\addcontentsline{toc}{section}{Versicherung}

\textbf{Name: }  Meyer

\vspace{.5cm}

\noindent \textbf{Vorname: } Timo 

\vspace{2cm}

\noindent
Ich versichere, dass ich diese Masterarbeit selbst\"andig verfasst und keine anderen als die angegebenen Quellen benutzt habe. \\
Die den benutzten Quellen w\"ortlich oder inhaltlich entnommenen Stellen habe ich als solche kenntlich gemacht. \\
Diese Versicherung gilt auch f\"ur alle gelieferten Datens\"atze, Zeichnungen, Skizzen oder grafischen Darstellungen.\\
Des Weiteren versichere ich, dass ich das Merkblatt zum Umgang mit Plagiaten (\href{http://phoenix.wiwi.uni-bielefeld.de/organisation/pamt/uploads/PlagiatInfo-BlattStudenten.pdf}{http://phoenix.wiwi.uni-bielefeld.de/organisation/pamt/uploads/PlagiatInfo-BlattStudenten.pdf})
gelesen habe.

\vspace{2cm} 


\begin{tabular}{lp{2em}l} 
Bielefeld, den \myformat\today    && \hspace{4cm} \\\cline{1-1}\cline{3-3} 
   && Unterschrift 
\end{tabular}  


% % % % % % % % % % % % % % % % % % % % % % % % 


\end{document}